* 
* ==> Audit <==
* |---------|---------------------|----------|-----------|---------|---------------------|---------------------|
| Command |        Args         | Profile  |   User    | Version |     Start Time      |      End Time       |
|---------|---------------------|----------|-----------|---------|---------------------|---------------------|
| start   |                     | minikube | parallels | v1.27.1 | 10 Oct 22 14:23 PDT |                     |
| start   |                     | minikube | parallels | v1.27.1 | 10 Oct 22 14:44 PDT | 10 Oct 22 14:48 PDT |
| kubectl | -- get pods -A      | minikube | parallels | v1.27.1 | 10 Oct 22 14:49 PDT | 10 Oct 22 14:49 PDT |
| start   |                     | minikube | parallels | v1.27.1 | 10 Oct 22 15:16 PDT | 10 Oct 22 15:17 PDT |
| start   |                     | minikube | parallels | v1.27.1 | 26 Oct 22 16:53 -03 | 26 Oct 22 16:57 -03 |
| start   |                     | minikube | parallels | v1.27.1 | 26 Oct 22 22:14 -03 | 26 Oct 22 22:16 -03 |
| service | msvc-usuarios --url | minikube | parallels | v1.27.1 | 27 Oct 22 13:21 -03 | 27 Oct 22 13:21 -03 |
| service | msvc-cursos --url   | minikube | parallels | v1.27.1 | 27 Oct 22 18:53 -03 | 27 Oct 22 18:53 -03 |
| start   |                     | minikube | parallels | v1.27.1 | 01 Nov 22 10:52 -03 | 01 Nov 22 10:53 -03 |
|---------|---------------------|----------|-----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/11/01 10:52:16
Running on machine: parallels-Parallels-Virtual-Platform
Binary: Built with gc go1.19.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1101 10:52:16.367800    7556 out.go:296] Setting OutFile to fd 1 ...
I1101 10:52:16.368045    7556 out.go:348] isatty.IsTerminal(1) = true
I1101 10:52:16.368049    7556 out.go:309] Setting ErrFile to fd 2...
I1101 10:52:16.368055    7556 out.go:348] isatty.IsTerminal(2) = true
I1101 10:52:16.368283    7556 root.go:333] Updating PATH: /home/parallels/.minikube/bin
W1101 10:52:16.368444    7556 root.go:310] Error reading config file at /home/parallels/.minikube/config/config.json: open /home/parallels/.minikube/config/config.json: no such file or directory
I1101 10:52:16.398184    7556 out.go:303] Setting JSON to false
I1101 10:52:16.399910    7556 start.go:116] hostinfo: {"hostname":"parallels-Parallels-Virtual-Platform","uptime":586,"bootTime":1667310150,"procs":255,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.0-52-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"842f139c-1c52-6d45-bafd-9f12e8b4c5f0"}
I1101 10:52:16.399984    7556 start.go:126] virtualization:  guest
I1101 10:52:16.401995    7556 out.go:177] 😄  minikube v1.27.1 en Ubuntu 22.04 (amd64)
I1101 10:52:16.403894    7556 notify.go:220] Checking for updates...
I1101 10:52:16.404017    7556 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I1101 10:52:16.421697    7556 driver.go:362] Setting default libvirt URI to qemu:///system
I1101 10:52:16.529277    7556 docker.go:137] docker version: linux-20.10.21
I1101 10:52:16.529378    7556 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1101 10:52:19.880810    7556 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (3.351406674s)
I1101 10:52:19.881152    7556 info.go:265] docker info: {ID:Z2EF:K4YW:CS4C:AMCD:3SXY:TIWW:P4HC:3H6K:3LGT:FVI4:CPB2:2GYG Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:26 OomKillDisable:false NGoroutines:38 SystemTime:2022-11-01 10:52:16.564641462 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:2 KernelVersion:5.15.0-52-generic OperatingSystem:Ubuntu 22.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8129568768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:parallels-Parallels-Virtual-Platform Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.2] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1101 10:52:19.881241    7556 docker.go:254] overlay module found
I1101 10:52:19.882401    7556 out.go:177] ✨  Using the docker driver based on existing profile
I1101 10:52:19.883291    7556 start.go:282] selected driver: docker
I1101 10:52:19.883298    7556 start.go:808] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/parallels:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1101 10:52:19.883383    7556 start.go:819] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1101 10:52:19.883489    7556 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1101 10:52:20.149173    7556 info.go:265] docker info: {ID:Z2EF:K4YW:CS4C:AMCD:3SXY:TIWW:P4HC:3H6K:3LGT:FVI4:CPB2:2GYG Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:26 OomKillDisable:false NGoroutines:38 SystemTime:2022-11-01 10:52:19.94843842 -0300 -03 LoggingDriver:json-file CgroupDriver:systemd NEventsListener:2 KernelVersion:5.15.0-52-generic OperatingSystem:Ubuntu 22.04.1 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8129568768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:parallels-Parallels-Virtual-Platform Labels:[] ExperimentalBuild:false ServerVersion:20.10.21 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:1c90a442489720eec95342e1789ee8a5e1b9536f Expected:1c90a442489720eec95342e1789ee8a5e1b9536f} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.12.2] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.21.0]] Warnings:<nil>}}
I1101 10:52:20.149569    7556 cni.go:95] Creating CNI manager for ""
I1101 10:52:20.149575    7556 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1101 10:52:20.149592    7556 start_flags.go:317] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/parallels:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1101 10:52:20.150629    7556 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1101 10:52:20.151289    7556 cache.go:120] Beginning downloading kic base image for docker with docker
I1101 10:52:20.151941    7556 out.go:177] 🚜  Pulling base image ...
I1101 10:52:20.152558    7556 preload.go:132] Checking if preload exists for k8s version v1.25.2 and runtime docker
I1101 10:52:20.152612    7556 preload.go:148] Found local preload: /home/parallels/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.2-docker-overlay2-amd64.tar.lz4
I1101 10:52:20.152618    7556 cache.go:57] Caching tarball of preloaded images
I1101 10:52:20.152674    7556 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b in local docker daemon
I1101 10:52:20.164520    7556 preload.go:174] Found /home/parallels/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.25.2-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1101 10:52:20.164541    7556 cache.go:60] Finished verifying existence of preloaded tar for  v1.25.2 on docker
I1101 10:52:20.164647    7556 profile.go:148] Saving config to /home/parallels/.minikube/profiles/minikube/config.json ...
I1101 10:52:20.209093    7556 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b in local docker daemon, skipping pull
I1101 10:52:20.209105    7556 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b exists in daemon, skipping load
I1101 10:52:20.209127    7556 cache.go:208] Successfully downloaded all kic artifacts
I1101 10:52:20.209216    7556 start.go:364] acquiring machines lock for minikube: {Name:mk32f34ba2114085e5caeaa748715e418d0ae106 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1101 10:52:20.215032    7556 start.go:368] acquired machines lock for "minikube" in 5.796811ms
I1101 10:52:20.215076    7556 start.go:96] Skipping create...Using existing machine configuration
I1101 10:52:20.215108    7556 fix.go:55] fixHost starting: 
I1101 10:52:20.215365    7556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1101 10:52:20.242701    7556 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1101 10:52:20.242714    7556 fix.go:129] unexpected machine state, will restart: <nil>
I1101 10:52:20.243813    7556 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1101 10:52:20.244609    7556 cli_runner.go:164] Run: docker start minikube
I1101 10:52:23.410558    7556 cli_runner.go:217] Completed: docker start minikube: (3.165924527s)
I1101 10:52:23.410624    7556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1101 10:52:23.436542    7556 kic.go:415] container "minikube" state is running.
I1101 10:52:23.437093    7556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1101 10:52:23.459969    7556 profile.go:148] Saving config to /home/parallels/.minikube/profiles/minikube/config.json ...
I1101 10:52:23.460178    7556 machine.go:88] provisioning docker machine ...
I1101 10:52:23.460224    7556 ubuntu.go:169] provisioning hostname "minikube"
I1101 10:52:23.460260    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:23.543088    7556 main.go:134] libmachine: Using SSH client type: native
I1101 10:52:23.560711    7556 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1101 10:52:23.560724    7556 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1101 10:52:23.561369    7556 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52250->127.0.0.1:49157: read: connection reset by peer
I1101 10:52:26.562213    7556 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52254->127.0.0.1:49157: read: connection reset by peer
I1101 10:52:29.565866    7556 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52264->127.0.0.1:49157: read: connection reset by peer
I1101 10:52:33.539266    7556 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1101 10:52:33.539315    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:33.591564    7556 main.go:134] libmachine: Using SSH client type: native
I1101 10:52:33.591727    7556 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1101 10:52:33.591737    7556 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1101 10:52:33.736893    7556 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1101 10:52:33.736938    7556 ubuntu.go:175] set auth options {CertDir:/home/parallels/.minikube CaCertPath:/home/parallels/.minikube/certs/ca.pem CaPrivateKeyPath:/home/parallels/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/parallels/.minikube/machines/server.pem ServerKeyPath:/home/parallels/.minikube/machines/server-key.pem ClientKeyPath:/home/parallels/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/parallels/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/parallels/.minikube}
I1101 10:52:33.736983    7556 ubuntu.go:177] setting up certificates
I1101 10:52:33.736988    7556 provision.go:83] configureAuth start
I1101 10:52:33.737048    7556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1101 10:52:33.785773    7556 provision.go:138] copyHostCerts
I1101 10:52:33.829551    7556 exec_runner.go:144] found /home/parallels/.minikube/ca.pem, removing ...
I1101 10:52:33.829579    7556 exec_runner.go:207] rm: /home/parallels/.minikube/ca.pem
I1101 10:52:33.829641    7556 exec_runner.go:151] cp: /home/parallels/.minikube/certs/ca.pem --> /home/parallels/.minikube/ca.pem (1086 bytes)
I1101 10:52:33.847448    7556 exec_runner.go:144] found /home/parallels/.minikube/cert.pem, removing ...
I1101 10:52:33.847456    7556 exec_runner.go:207] rm: /home/parallels/.minikube/cert.pem
I1101 10:52:33.847487    7556 exec_runner.go:151] cp: /home/parallels/.minikube/certs/cert.pem --> /home/parallels/.minikube/cert.pem (1127 bytes)
I1101 10:52:33.850541    7556 exec_runner.go:144] found /home/parallels/.minikube/key.pem, removing ...
I1101 10:52:33.850548    7556 exec_runner.go:207] rm: /home/parallels/.minikube/key.pem
I1101 10:52:33.850580    7556 exec_runner.go:151] cp: /home/parallels/.minikube/certs/key.pem --> /home/parallels/.minikube/key.pem (1679 bytes)
I1101 10:52:33.864544    7556 provision.go:112] generating server cert: /home/parallels/.minikube/machines/server.pem ca-key=/home/parallels/.minikube/certs/ca.pem private-key=/home/parallels/.minikube/certs/ca-key.pem org=parallels.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1101 10:52:34.164126    7556 provision.go:172] copyRemoteCerts
I1101 10:52:34.164225    7556 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1101 10:52:34.164263    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:34.193070    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:52:34.287509    7556 ssh_runner.go:362] scp /home/parallels/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I1101 10:52:34.396488    7556 ssh_runner.go:362] scp /home/parallels/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I1101 10:52:34.414654    7556 ssh_runner.go:362] scp /home/parallels/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1101 10:52:34.432542    7556 provision.go:86] duration metric: configureAuth took 695.544567ms
I1101 10:52:34.432556    7556 ubuntu.go:193] setting minikube options for container-runtime
I1101 10:52:34.432715    7556 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
I1101 10:52:34.432754    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:34.506016    7556 main.go:134] libmachine: Using SSH client type: native
I1101 10:52:34.506117    7556 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1101 10:52:34.506122    7556 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1101 10:52:34.725069    7556 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1101 10:52:34.725092    7556 ubuntu.go:71] root file system type: overlay
I1101 10:52:34.725206    7556 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1101 10:52:34.725251    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:34.749170    7556 main.go:134] libmachine: Using SSH client type: native
I1101 10:52:34.749278    7556 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1101 10:52:34.749324    7556 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1101 10:52:34.880833    7556 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1101 10:52:34.880884    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:34.907287    7556 main.go:134] libmachine: Using SSH client type: native
I1101 10:52:34.907404    7556 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7ecce0] 0x7efe60 <nil>  [] 0s} 127.0.0.1 49157 <nil> <nil>}
I1101 10:52:34.907415    7556 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1101 10:52:35.083175    7556 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1101 10:52:35.083190    7556 machine.go:91] provisioned docker machine in 11.62300694s
I1101 10:52:35.083216    7556 start.go:300] post-start starting for "minikube" (driver="docker")
I1101 10:52:35.083225    7556 start.go:328] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1101 10:52:35.083299    7556 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1101 10:52:35.083340    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:35.111956    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:52:35.252736    7556 ssh_runner.go:195] Run: cat /etc/os-release
I1101 10:52:35.256548    7556 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1101 10:52:35.256560    7556 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1101 10:52:35.256567    7556 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1101 10:52:35.256573    7556 info.go:137] Remote host: Ubuntu 20.04.5 LTS
I1101 10:52:35.256598    7556 filesync.go:126] Scanning /home/parallels/.minikube/addons for local assets ...
I1101 10:52:35.268550    7556 filesync.go:126] Scanning /home/parallels/.minikube/files for local assets ...
I1101 10:52:35.275829    7556 start.go:303] post-start completed in 192.596011ms
I1101 10:52:35.275939    7556 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1101 10:52:35.276003    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:35.318678    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:52:35.488329    7556 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1101 10:52:35.493734    7556 fix.go:57] fixHost completed within 15.278631818s
I1101 10:52:35.493748    7556 start.go:83] releasing machines lock for "minikube", held for 15.278703171s
I1101 10:52:35.493833    7556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1101 10:52:35.533149    7556 ssh_runner.go:195] Run: systemctl --version
I1101 10:52:35.533213    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:35.533235    7556 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1101 10:52:35.548669    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:52:35.580393    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:52:35.608921    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:52:36.192411    7556 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1101 10:52:37.922937    7556 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.389678898s)
I1101 10:52:37.923074    7556 ssh_runner.go:235] Completed: sudo systemctl cat docker.service: (1.730648462s)
I1101 10:52:37.923099    7556 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1101 10:52:37.923151    7556 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1101 10:52:38.011271    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1101 10:52:38.025974    7556 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1101 10:52:38.202301    7556 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1101 10:52:38.315859    7556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1101 10:52:38.421297    7556 ssh_runner.go:195] Run: sudo systemctl restart docker
I1101 10:52:42.637061    7556 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.215746419s)
I1101 10:52:42.637103    7556 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1101 10:52:42.911449    7556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1101 10:52:43.019894    7556 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1101 10:52:43.029287    7556 start.go:451] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1101 10:52:43.029343    7556 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1101 10:52:43.033075    7556 start.go:472] Will wait 60s for crictl version
I1101 10:52:43.033114    7556 ssh_runner.go:195] Run: sudo crictl version
I1101 10:52:45.184821    7556 ssh_runner.go:235] Completed: sudo crictl version: (2.151691859s)
I1101 10:52:45.184841    7556 start.go:481] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.18
RuntimeApiVersion:  1.41.0
I1101 10:52:45.184880    7556 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1101 10:52:46.004832    7556 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1101 10:52:46.039982    7556 out.go:204] 🐳  Preparando Kubernetes v1.25.2 en Docker 20.10.18...
I1101 10:52:46.078989    7556 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1101 10:52:46.122284    7556 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1101 10:52:46.142775    7556 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1101 10:52:46.299144    7556 preload.go:132] Checking if preload exists for k8s version v1.25.2 and runtime docker
I1101 10:52:46.299331    7556 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1101 10:52:46.333891    7556 docker.go:611] Got preloaded images: -- stdout --
rorolopezg/usuarios:latest
rorolopezg/usuarios:v2
rorolopezg/usuarios:<none>
mysql:8
<none>:<none>
postgres:14-alpine
rorolopezg/cursos:latest
<none>:<none>
registry.k8s.io/kube-apiserver:v1.25.2
registry.k8s.io/kube-controller-manager:v1.25.2
registry.k8s.io/kube-scheduler:v1.25.2
registry.k8s.io/kube-proxy:v1.25.2
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1101 10:52:46.333905    7556 docker.go:542] Images already preloaded, skipping extraction
I1101 10:52:46.334022    7556 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1101 10:52:46.376979    7556 docker.go:611] Got preloaded images: -- stdout --
rorolopezg/usuarios:latest
rorolopezg/usuarios:v2
rorolopezg/usuarios:<none>
mysql:8
<none>:<none>
postgres:14-alpine
rorolopezg/cursos:latest
<none>:<none>
registry.k8s.io/kube-apiserver:v1.25.2
registry.k8s.io/kube-scheduler:v1.25.2
registry.k8s.io/kube-controller-manager:v1.25.2
registry.k8s.io/kube-proxy:v1.25.2
registry.k8s.io/pause:3.8
registry.k8s.io/etcd:3.5.4-0
registry.k8s.io/coredns/coredns:v1.9.3
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1101 10:52:46.377026    7556 cache_images.go:84] Images are preloaded, skipping loading
I1101 10:52:46.377096    7556 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1101 10:52:49.780212    7556 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (3.40309916s)
I1101 10:52:49.780252    7556 cni.go:95] Creating CNI manager for ""
I1101 10:52:49.780263    7556 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1101 10:52:49.780306    7556 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1101 10:52:49.780318    7556 kubeadm.go:156] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.25.2 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false}
I1101 10:52:49.780417    7556 kubeadm.go:161] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.25.2
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1101 10:52:49.795935    7556 kubeadm.go:962] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.25.2/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.25.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1101 10:52:49.795995    7556 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.25.2
I1101 10:52:49.843004    7556 binaries.go:44] Found k8s binaries, skipping transfer
I1101 10:52:49.843059    7556 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1101 10:52:49.892097    7556 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1101 10:52:49.927484    7556 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1101 10:52:49.942239    7556 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I1101 10:52:49.975151    7556 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1101 10:52:49.978577    7556 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1101 10:52:49.988413    7556 certs.go:54] Setting up /home/parallels/.minikube/profiles/minikube for IP: 192.168.49.2
I1101 10:52:49.988497    7556 certs.go:182] skipping minikubeCA CA generation: /home/parallels/.minikube/ca.key
I1101 10:52:50.004098    7556 certs.go:182] skipping proxyClientCA CA generation: /home/parallels/.minikube/proxy-client-ca.key
I1101 10:52:50.004244    7556 certs.go:298] skipping minikube-user signed cert generation: /home/parallels/.minikube/profiles/minikube/client.key
I1101 10:52:50.028129    7556 certs.go:298] skipping minikube signed cert generation: /home/parallels/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1101 10:52:50.035211    7556 certs.go:298] skipping aggregator signed cert generation: /home/parallels/.minikube/profiles/minikube/proxy-client.key
I1101 10:52:50.035355    7556 certs.go:388] found cert: /home/parallels/.minikube/certs/home/parallels/.minikube/certs/ca-key.pem (1679 bytes)
I1101 10:52:50.035380    7556 certs.go:388] found cert: /home/parallels/.minikube/certs/home/parallels/.minikube/certs/ca.pem (1086 bytes)
I1101 10:52:50.035399    7556 certs.go:388] found cert: /home/parallels/.minikube/certs/home/parallels/.minikube/certs/cert.pem (1127 bytes)
I1101 10:52:50.035415    7556 certs.go:388] found cert: /home/parallels/.minikube/certs/home/parallels/.minikube/certs/key.pem (1679 bytes)
I1101 10:52:50.036100    7556 ssh_runner.go:362] scp /home/parallels/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1101 10:52:50.101558    7556 ssh_runner.go:362] scp /home/parallels/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1101 10:52:50.135075    7556 ssh_runner.go:362] scp /home/parallels/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1101 10:52:50.162369    7556 ssh_runner.go:362] scp /home/parallels/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1101 10:52:50.192756    7556 ssh_runner.go:362] scp /home/parallels/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1101 10:52:50.229521    7556 ssh_runner.go:362] scp /home/parallels/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1101 10:52:50.263744    7556 ssh_runner.go:362] scp /home/parallels/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1101 10:52:50.300127    7556 ssh_runner.go:362] scp /home/parallels/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1101 10:52:50.338267    7556 ssh_runner.go:362] scp /home/parallels/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1101 10:52:50.389135    7556 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1101 10:52:50.412042    7556 ssh_runner.go:195] Run: openssl version
I1101 10:52:50.459403    7556 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1101 10:52:50.478785    7556 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1101 10:52:50.482484    7556 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Oct 10 21:47 /usr/share/ca-certificates/minikubeCA.pem
I1101 10:52:50.482523    7556 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1101 10:52:50.522320    7556 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1101 10:52:50.529973    7556 kubeadm.go:396] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.35@sha256:e6f9b2700841634f3b94907f9cfa6785ca4409ef8e428a0322c1781e809d311b Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.25.2 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/parallels:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath:/opt/socket_vmnet/bin/socket_vmnet_client SocketVMnetPath:/var/run/socket_vmnet}
I1101 10:52:50.530086    7556 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1101 10:52:50.572996    7556 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1101 10:52:50.603414    7556 kubeadm.go:411] found existing configuration files, will attempt cluster restart
I1101 10:52:50.603513    7556 kubeadm.go:627] restartCluster start
I1101 10:52:50.603558    7556 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1101 10:52:50.619626    7556 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1101 10:52:50.620453    7556 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I1101 10:52:50.684567    7556 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1101 10:52:50.724922    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:50.724964    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:50.758145    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:50.958908    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:50.958970    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:50.967977    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:51.158388    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:51.158438    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:51.169191    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:51.358656    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:51.358711    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:51.367243    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:51.558751    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:51.558831    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:51.567320    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:51.758931    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:51.758992    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:51.767703    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:51.958904    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:51.958965    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:51.968178    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:52.158990    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:52.159040    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:52.169606    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:52.358767    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:52.358820    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:52.367338    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:52.559045    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:52.559092    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:52.567585    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:52.758414    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:52.758476    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:52.767279    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:52.958212    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:52.958299    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:52.967686    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.159316    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:53.159401    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:53.170286    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.358788    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:53.358859    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:53.368434    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.558890    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:53.558938    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:53.568394    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.771208    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:53.771260    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:53.780676    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.780684    7556 api_server.go:165] Checking apiserver status ...
I1101 10:52:53.780725    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1101 10:52:53.789115    7556 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.789127    7556 kubeadm.go:602] needs reconfigure: apiserver error: timed out waiting for the condition
I1101 10:52:53.789131    7556 kubeadm.go:1114] stopping kube-system containers ...
I1101 10:52:53.789172    7556 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1101 10:52:53.819091    7556 docker.go:443] Stopping containers: [87074a63a0e2 7d0850c601f0 41345a04da82 72df13b05e0f 3a4d4acfe785 0f833ac603d6 47fc93a053f2 af902fca7879 c04a84748727 87e47d44e685 208a5171c120 174f5c565f5e 50b0eea0f50a 2f5cd3091f71 e1a92b23f6f2 7dd78760e55e 0bbccc4947e9 85b010679a93 d1aa0ceec953 8621721932ac c916788d6c5b 0cc92ed8649a c8897158ee2b add7e5fdf522 9846e697fe94 f3a616b8d11a 11419c2255d6]
I1101 10:52:53.819143    7556 ssh_runner.go:195] Run: docker stop 87074a63a0e2 7d0850c601f0 41345a04da82 72df13b05e0f 3a4d4acfe785 0f833ac603d6 47fc93a053f2 af902fca7879 c04a84748727 87e47d44e685 208a5171c120 174f5c565f5e 50b0eea0f50a 2f5cd3091f71 e1a92b23f6f2 7dd78760e55e 0bbccc4947e9 85b010679a93 d1aa0ceec953 8621721932ac c916788d6c5b 0cc92ed8649a c8897158ee2b add7e5fdf522 9846e697fe94 f3a616b8d11a 11419c2255d6
I1101 10:52:53.867607    7556 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1101 10:52:53.900735    7556 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1101 10:52:53.908357    7556 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Oct 26 19:56 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Oct 27 01:15 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 5659 Oct 26 19:56 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Oct 27 01:15 /etc/kubernetes/scheduler.conf

I1101 10:52:53.908402    7556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1101 10:52:53.927771    7556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1101 10:52:53.944437    7556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1101 10:52:53.969745    7556 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.969785    7556 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1101 10:52:53.977698    7556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1101 10:52:53.989330    7556 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1101 10:52:53.989374    7556 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1101 10:52:53.997576    7556 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1101 10:52:54.005832    7556 kubeadm.go:704] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1101 10:52:54.005849    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1101 10:52:55.417644    7556 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (1.411750578s)
I1101 10:52:55.417659    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1101 10:52:56.267394    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1101 10:52:56.862213    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1101 10:52:56.970595    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1101 10:52:57.045672    7556 api_server.go:51] waiting for apiserver process to appear ...
I1101 10:52:57.045732    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:52:57.563124    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:52:58.063859    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:52:58.563041    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:52:59.062793    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:52:59.563589    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:00.062896    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:00.563516    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:01.063512    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:01.562939    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:02.063700    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:02.563203    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:03.063900    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:03.563768    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:04.062726    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:04.563772    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:05.062763    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:05.074213    7556 api_server.go:71] duration metric: took 8.028546463s to wait for apiserver process to appear ...
I1101 10:53:05.074226    7556 api_server.go:87] waiting for apiserver healthz status ...
I1101 10:53:05.074235    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:05.074531    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:05.575069    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:05.575419    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:06.075636    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:06.076080    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:06.575282    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:06.575730    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:07.075264    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:07.075554    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:07.575722    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:07.576124    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:08.075337    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:08.075623    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:08.582080    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:08.582464    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:09.075698    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:09.076129    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:09.577589    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:09.577895    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:10.075030    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:10.075502    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:10.575902    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:10.576410    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:11.075478    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:11.076021    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:11.575269    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:11.575724    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:12.075581    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:12.075919    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:12.580766    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:12.581220    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:13.074907    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:13.075317    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:13.575400    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:13.575817    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:14.077387    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:14.077777    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:14.575312    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:14.575792    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:15.074738    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:15.075249    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:15.616085    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:15.616466    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:16.075081    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:16.075743    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:16.575068    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:16.575626    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:17.075471    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:17.075996    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:17.575298    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:17.575768    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:18.075339    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:18.075762    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:18.575127    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:18.575587    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:19.075505    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:19.075945    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:19.575291    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:19.575618    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:20.075480    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:20.076041    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:20.575163    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:20.575516    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I1101 10:53:21.075260    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:26.088073    7556 api_server.go:268] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1101 10:53:26.575805    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:27.934507    7556 api_server.go:278] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1101 10:53:27.934526    7556 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1101 10:53:28.075989    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:28.123205    7556 api_server.go:278] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1101 10:53:28.123223    7556 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1101 10:53:28.575812    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:28.581880    7556 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1101 10:53:28.581896    7556 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1101 10:53:29.075337    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:29.081233    7556 api_server.go:278] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1101 10:53:29.081249    7556 api_server.go:102] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1101 10:53:29.596031    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:29.602046    7556 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1101 10:53:29.672070    7556 api_server.go:140] control plane version: v1.25.2
I1101 10:53:29.672088    7556 api_server.go:130] duration metric: took 24.597856466s to wait for apiserver health ...
I1101 10:53:29.672127    7556 cni.go:95] Creating CNI manager for ""
I1101 10:53:29.672136    7556 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1101 10:53:29.672169    7556 system_pods.go:43] waiting for kube-system pods to appear ...
I1101 10:53:29.702392    7556 system_pods.go:59] 7 kube-system pods found
I1101 10:53:29.702429    7556 system_pods.go:61] "coredns-565d847f94-r4f4l" [5515731c-cfee-4af0-a227-3bf01279d481] Running
I1101 10:53:29.702434    7556 system_pods.go:61] "etcd-minikube" [5485e622-9382-4385-ad0f-1b39aefe4be1] Running
I1101 10:53:29.702439    7556 system_pods.go:61] "kube-apiserver-minikube" [4bd1c982-812d-4eb7-a9c9-bb962cbc6419] Running
I1101 10:53:29.702443    7556 system_pods.go:61] "kube-controller-manager-minikube" [c1418366-782d-4f4f-84b6-10b54a2d5c64] Running
I1101 10:53:29.702448    7556 system_pods.go:61] "kube-proxy-cgglk" [7550851a-626d-4cf4-950f-c35337808b09] Running
I1101 10:53:29.702452    7556 system_pods.go:61] "kube-scheduler-minikube" [95a333e5-0b3b-4ebb-9781-b924d07872ce] Running
I1101 10:53:29.702456    7556 system_pods.go:61] "storage-provisioner" [c64c1953-f284-44d6-9c82-27cfc6cda6c9] Running
I1101 10:53:29.702461    7556 system_pods.go:74] duration metric: took 30.286419ms to wait for pod list to return data ...
I1101 10:53:29.702468    7556 node_conditions.go:102] verifying NodePressure condition ...
I1101 10:53:29.706593    7556 node_conditions.go:122] node storage ephemeral capacity is 65738284Ki
I1101 10:53:29.706631    7556 node_conditions.go:123] node cpu capacity is 4
I1101 10:53:29.706657    7556 node_conditions.go:105] duration metric: took 4.1837ms to run NodePressure ...
I1101 10:53:29.706678    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.25.2:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1101 10:53:30.397186    7556 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1101 10:53:30.430337    7556 ops.go:34] apiserver oom_adj: -16
I1101 10:53:30.430349    7556 kubeadm.go:631] restartCluster took 39.826829739s
I1101 10:53:30.430356    7556 kubeadm.go:398] StartCluster complete in 39.900402342s
I1101 10:53:30.430418    7556 settings.go:142] acquiring lock: {Name:mk593ea99ac655b6ef339eed85fbcd9fbedf95c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1101 10:53:30.430624    7556 settings.go:150] Updating kubeconfig:  /home/parallels/.kube/config
I1101 10:53:30.432392    7556 lock.go:35] WriteFile acquiring /home/parallels/.kube/config: {Name:mk543bc23b0f05391d20f0ec655c9bc6995b7f79 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1101 10:53:30.476553    7556 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1101 10:53:30.476673    7556 start.go:212] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.25.2 ContainerRuntime:docker ControlPlane:true Worker:true}
I1101 10:53:30.476717    7556 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.2/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1101 10:53:30.539319    7556 out.go:177] 🔎  Verifying Kubernetes components...
I1101 10:53:30.476880    7556 addons.go:412] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I1101 10:53:30.539459    7556 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1101 10:53:30.539493    7556 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I1101 10:53:30.477168    7556 config.go:180] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.25.2
W1101 10:53:30.539500    7556 addons.go:162] addon storage-provisioner should already be in state true
I1101 10:53:30.539553    7556 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1101 10:53:30.539570    7556 host.go:66] Checking if "minikube" exists ...
I1101 10:53:30.539604    7556 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1101 10:53:30.540125    7556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1101 10:53:30.540424    7556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1101 10:53:30.645867    7556 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1101 10:53:31.425129    7556 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1101 10:53:31.530302    7556 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1101 10:53:31.534021    7556 addons.go:162] addon default-storageclass should already be in state true
I1101 10:53:31.534063    7556 host.go:66] Checking if "minikube" exists ...
I1101 10:53:31.534172    7556 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1101 10:53:31.534183    7556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1101 10:53:31.534340    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:53:31.534698    7556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1101 10:53:31.590275    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:53:31.604955    7556 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I1101 10:53:31.604980    7556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1101 10:53:31.605196    7556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1101 10:53:31.771228    7556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49157 SSHKeyPath:/home/parallels/.minikube/machines/minikube/id_rsa Username:docker}
I1101 10:53:32.245799    7556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1101 10:53:32.248698    7556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1101 10:53:33.504781    7556 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.25.2/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (3.028041019s)
I1101 10:53:33.504831    7556 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (2.858937384s)
I1101 10:53:33.504872    7556 api_server.go:51] waiting for apiserver process to appear ...
I1101 10:53:33.504899    7556 start.go:806] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1101 10:53:33.504940    7556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1101 10:53:34.254451    7556 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.008617181s)
I1101 10:53:34.287454    7556 api_server.go:71] duration metric: took 3.810752684s to wait for apiserver process to appear ...
I1101 10:53:34.287467    7556 api_server.go:87] waiting for apiserver healthz status ...
I1101 10:53:34.287476    7556 api_server.go:252] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1101 10:53:34.287555    7556 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.25.2/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.038827531s)
I1101 10:53:34.290532    7556 out.go:177] 🌟  Complementos habilitados: default-storageclass, storage-provisioner
I1101 10:53:34.292375    7556 addons.go:414] enableAddons completed in 3.815566056s
I1101 10:53:34.294129    7556 api_server.go:278] https://192.168.49.2:8443/healthz returned 200:
ok
I1101 10:53:34.295725    7556 api_server.go:140] control plane version: v1.25.2
I1101 10:53:34.295744    7556 api_server.go:130] duration metric: took 8.269891ms to wait for apiserver health ...
I1101 10:53:34.295754    7556 system_pods.go:43] waiting for kube-system pods to appear ...
I1101 10:53:34.332011    7556 system_pods.go:59] 7 kube-system pods found
I1101 10:53:34.332029    7556 system_pods.go:61] "coredns-565d847f94-r4f4l" [5515731c-cfee-4af0-a227-3bf01279d481] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1101 10:53:34.332034    7556 system_pods.go:61] "etcd-minikube" [5485e622-9382-4385-ad0f-1b39aefe4be1] Running
I1101 10:53:34.332040    7556 system_pods.go:61] "kube-apiserver-minikube" [4bd1c982-812d-4eb7-a9c9-bb962cbc6419] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1101 10:53:34.332046    7556 system_pods.go:61] "kube-controller-manager-minikube" [c1418366-782d-4f4f-84b6-10b54a2d5c64] Running
I1101 10:53:34.332052    7556 system_pods.go:61] "kube-proxy-cgglk" [7550851a-626d-4cf4-950f-c35337808b09] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1101 10:53:34.332056    7556 system_pods.go:61] "kube-scheduler-minikube" [95a333e5-0b3b-4ebb-9781-b924d07872ce] Running
I1101 10:53:34.332061    7556 system_pods.go:61] "storage-provisioner" [c64c1953-f284-44d6-9c82-27cfc6cda6c9] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1101 10:53:34.332068    7556 system_pods.go:74] duration metric: took 36.309583ms to wait for pod list to return data ...
I1101 10:53:34.332077    7556 kubeadm.go:573] duration metric: took 3.855379461s to wait for : map[apiserver:true system_pods:true] ...
I1101 10:53:34.332088    7556 node_conditions.go:102] verifying NodePressure condition ...
I1101 10:53:34.335152    7556 node_conditions.go:122] node storage ephemeral capacity is 65738284Ki
I1101 10:53:34.335164    7556 node_conditions.go:123] node cpu capacity is 4
I1101 10:53:34.335172    7556 node_conditions.go:105] duration metric: took 3.080121ms to run NodePressure ...
I1101 10:53:34.335182    7556 start.go:217] waiting for startup goroutines ...
I1101 10:53:34.335896    7556 ssh_runner.go:195] Run: rm -f paused
I1101 10:53:37.320100    7556 start.go:506] kubectl: 1.23.3, cluster: 1.25.2 (minor skew: 2)
I1101 10:53:37.348759    7556 out.go:177] 
W1101 10:53:37.408960    7556 out.go:239] ❗  /usr/bin/kubectl is version 1.23.3, which may have incompatibilites with Kubernetes 1.25.2.
I1101 10:53:37.474071    7556 out.go:177]     ▪ Want kubectl v1.25.2? Try 'minikube kubectl -- get pods -A'
I1101 10:53:37.531257    7556 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Tue 2022-11-01 13:52:28 UTC, end at Tue 2022-11-01 13:56:48 UTC. --
Nov 01 13:52:31 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.028891712Z" level=info msg="Starting up"
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.109093129Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.109145523Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.109249734Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.109276260Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.218024495Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.218070804Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.218102014Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.218113017Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Nov 01 13:52:34 minikube dockerd[132]: time="2022-11-01T13:52:34.514163886Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Nov 01 13:52:38 minikube dockerd[132]: time="2022-11-01T13:52:38.430434111Z" level=info msg="Processing signal 'terminated'"
Nov 01 13:52:39 minikube dockerd[132]: time="2022-11-01T13:52:39.161357497Z" level=info msg="Loading containers: start."
Nov 01 13:52:40 minikube dockerd[132]: time="2022-11-01T13:52:40.904528558Z" level=info msg="Removing stale sandbox 0cb0ba3f3c061e1f50c8de2032dd3bb789e204d4a0a3aba81434364819cd466d (85e181af42f80a1b26f8b5c9e2e1c1708fba21a9d9ab7c0d0e7e93770c4a8073)"
Nov 01 13:52:40 minikube dockerd[132]: time="2022-11-01T13:52:40.910191799Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 557fc802cf85e407fbcf80a9e8fe0c4e5d31f156a9cd1bf60f79ea3918840183 eb0f43f28566d7b90397ffb5b48a79a7387fbb244eadf9a2f572ad204565cb67], retrying...."
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.218057519Z" level=info msg="Removing stale sandbox 49f2c7dfd3c2f80d5172813cab8d559ea76b5a937b5f8ebe6bdb7e02ae937adf (41345a04da822bae7e2b1052a5b32ffd8b938b5f7ddc0aa6be3701dadaab35b6)"
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.222969016Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 557fc802cf85e407fbcf80a9e8fe0c4e5d31f156a9cd1bf60f79ea3918840183 a22191b291f4c06e3eba8bb9130e917e3560b2fb482072ff5dc1d161407b175f], retrying...."
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.317875944Z" level=info msg="Removing stale sandbox 629c92d9c7f18da7501ad0a8e77cdeca11ffac7e3645663b89623e9a76b3f6da (42586e801b940a54e2a97973a675383d38207efee613f7bf6dbd77b25e139ccc)"
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.323094364Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 557fc802cf85e407fbcf80a9e8fe0c4e5d31f156a9cd1bf60f79ea3918840183 d890c71e1890a88f067e23c9cf24020059c8c18d0adbdaf299d7b168fa124ff4], retrying...."
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.408834986Z" level=info msg="Removing stale sandbox b867e44a27cd0a26e6920da121f46e5ccd8c217952b64b00f76536ce6791fbdc (174f5c565f5e4ba368b4da9c4a713fb4577312f1b8951e740e794cec1c65a133)"
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.410052755Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint 64bf477e4b92531087fa718efeb38162ac75433b1eb00fb1474ff6a6ed956af1 08b19170764d0e77fbad9dea37bb055a0e8a749889b7b57c1cd90bb9706d25f6], retrying...."
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.441615658Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.559729477Z" level=info msg="Loading containers: done."
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.892174362Z" level=info msg="Docker daemon" commit=e42327a graphdriver(s)=overlay2 version=20.10.18
Nov 01 13:52:41 minikube dockerd[132]: time="2022-11-01T13:52:41.910972330Z" level=info msg="Daemon has completed initialization"
Nov 01 13:52:42 minikube dockerd[132]: time="2022-11-01T13:52:42.084819110Z" level=info msg="API listen on [::]:2376"
Nov 01 13:52:42 minikube dockerd[132]: time="2022-11-01T13:52:42.090132669Z" level=info msg="API listen on /var/run/docker.sock"
Nov 01 13:52:42 minikube dockerd[132]: time="2022-11-01T13:52:42.091278548Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Nov 01 13:52:42 minikube dockerd[132]: time="2022-11-01T13:52:42.091830996Z" level=info msg="Daemon shutdown complete"
Nov 01 13:52:42 minikube dockerd[132]: time="2022-11-01T13:52:42.091867760Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Nov 01 13:52:42 minikube systemd[1]: docker.service: Succeeded.
Nov 01 13:52:42 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 01 13:52:42 minikube systemd[1]: docker.service: Consumed 1.935s CPU time.
Nov 01 13:52:42 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.207844281Z" level=info msg="Starting up"
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.209531339Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.209550935Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.209567844Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.209575741Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.210758462Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.210787110Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.210799150Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.210806730Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.214613996Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.222961878Z" level=info msg="Loading containers: start."
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.466642106Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.601124344Z" level=info msg="Loading containers: done."
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.622195283Z" level=info msg="Docker daemon" commit=e42327a graphdriver(s)=overlay2 version=20.10.18
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.622313063Z" level=info msg="Daemon has completed initialization"
Nov 01 13:52:42 minikube systemd[1]: Started Docker Application Container Engine.
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.639242572Z" level=info msg="API listen on [::]:2376"
Nov 01 13:52:42 minikube dockerd[508]: time="2022-11-01T13:52:42.643465117Z" level=info msg="API listen on /var/run/docker.sock"
Nov 01 13:54:22 minikube dockerd[508]: time="2022-11-01T13:54:22.687826550Z" level=info msg="ignoring event" container=7c168047037b7ee03948770c7ed24b0349a05e56ed2b5e629b81f5c86dac7372 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID
f9f531c44e928       6e38f40d628db       About a minute ago   Running             storage-provisioner       17                  1167002b9f3d7
7c168047037b7       6e38f40d628db       3 minutes ago        Exited              storage-provisioner       16                  1167002b9f3d7
6bb1b0f8193bd       1c7d8c51823b5       3 minutes ago        Running             kube-proxy                4                   c4d2557dbb0fb
77888e7cdee8f       8fad08b3c84be       3 minutes ago        Running             mysql                     1                   0fdf4baa4c44f
53b78b08a38cb       5185b96f0becf       3 minutes ago        Running             coredns                   4                   865321f5bf087
eb5bfa50d4ff6       aac01494762a1       3 minutes ago        Running             postgres14                2                   bd51e348bca75
5460ec2ef5e9a       dbfceb93c69b6       3 minutes ago        Running             kube-controller-manager   4                   97776cd586526
259c449a294fd       a8a176a5d5d69       3 minutes ago        Running             etcd                      4                   3b9ca212556ab
50dea40ba47e3       ca0ea1ee3cfd3       3 minutes ago        Running             kube-scheduler            4                   ea34132d343c8
7f99ff807be69       97801f8394908       3 minutes ago        Running             kube-apiserver            5                   246056af2b5cf
5598d642bbc22       aac01494762a1       4 days ago           Exited              postgres14                1                   85e181af42f80
29c56f864d352       8fad08b3c84be       4 days ago           Exited              mysql                     0                   42586e801b940
7d0850c601f0c       5185b96f0becf       5 days ago           Exited              coredns                   3                   41345a04da822
3a4d4acfe7851       1c7d8c51823b5       5 days ago           Exited              kube-proxy                3                   47fc93a053f2a
af902fca78794       dbfceb93c69b6       5 days ago           Exited              kube-controller-manager   3                   2f5cd3091f71e
c04a847487276       97801f8394908       5 days ago           Exited              kube-apiserver            4                   e1a92b23f6f23
87e47d44e685a       a8a176a5d5d69       5 days ago           Exited              etcd                      3                   174f5c565f5e4
208a5171c120c       ca0ea1ee3cfd3       5 days ago           Exited              kube-scheduler            3                   50b0eea0f50ae

* 
* ==> coredns [53b78b08a38c] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:34596->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:46667->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:45511->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:35437->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:39549->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:55835->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:54996->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 7114243323297368514.2879202664022365806. HINFO: read udp 172.17.0.2:49324->192.168.49.1:53: i/o timeout
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [7d0850c601f0] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = eff20e86b4fd2b9878e9c34205d7ba141ff41613cbdadb71e63d4a8be6caff7d1fbccef3edfe618baf8958049a58d98ae28ea781e3e7cdf1cc90820da8e01a6d
CoreDNS-1.9.3
linux/amd64, go1.18.2, 45b0a11
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:47024->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:50959->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:40364->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:56544->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:56052->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:60462->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:60001->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:58129->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:52898->192.168.49.1:53: i/o timeout
[ERROR] plugin/errors: 2 8799754633161133725.480208937665634838. HINFO: read udp 172.17.0.3:39618->192.168.49.1:53: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.00385196s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.091702113s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.991602198s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fe869b5d4da11ba318eb84a3ac00f336411de7ba
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_10_10T14_47_57_0700
                    minikube.k8s.io/version=v1.27.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 10 Oct 2022 21:47:56 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 01 Nov 2022 13:56:42 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 01 Nov 2022 13:53:28 +0000   Mon, 10 Oct 2022 22:03:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 01 Nov 2022 13:53:28 +0000   Mon, 10 Oct 2022 22:03:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 01 Nov 2022 13:53:28 +0000   Mon, 10 Oct 2022 22:03:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 01 Nov 2022 13:53:28 +0000   Mon, 10 Oct 2022 22:03:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  65738284Ki
  hugepages-2Mi:      0
  memory:             7939032Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  65738284Ki
  hugepages-2Mi:      0
  memory:             7939032Ki
  pods:               110
System Info:
  Machine ID:                 386775ffbc46464ea356d784cd8cd54c
  System UUID:                01d6e76f-3feb-4a92-bf50-600087fbad45
  Boot ID:                    838c0bc5-7ffa-4fc0-8fa7-4f66ca1c8ed2
  Kernel Version:             5.15.0-52-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.18
  Kubelet Version:            v1.25.2
  Kube-Proxy Version:         v1.25.2
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     mysql8-66649767dd-rkzjt             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d2h
  default                     postgres14-6dd94f9766-d876l         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d2h
  kube-system                 coredns-565d847f94-r4f4l            100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     21d
  kube-system                 etcd-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         21d
  kube-system                 kube-apiserver-minikube             250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
  kube-system                 kube-controller-manager-minikube    200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
  kube-system                 kube-proxy-cgglk                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
  kube-system                 kube-scheduler-minikube             100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         21d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2m53s                  kube-proxy       
  Normal  Starting                 3m49s                  kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  3m49s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  3m48s (x8 over 3m49s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m48s (x8 over 3m49s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m48s (x7 over 3m49s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           3m9s                   node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Nov 1 13:42] RETBleed: WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!
[  +0.024578]   #2
[  +0.003867]   #3
[  +0.042975] pmd_set_huge: Cannot satisfy [mem 0xfc000000-0xfc200000] with a huge-page mapping due to MTRR override.
[  +0.189861] ACPI: \_SB_.PCI0.LNKA: _CRS evaluation failed: AE_NOT_FOUND
[  +0.301774] ehci-pci 0000:00:1d.7: Enabling legacy PCI PM
[  +0.032737] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000593] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000012] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000012] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000012] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000011] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000011] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000011] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000041] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000013] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.965451] ACPI Warning: SystemIO range 0x0000000000004028-0x000000000000402F conflicts with OpRegion 0x0000000000004000-0x000000000000404E (\PMIO) (20210730/utaddress-204)
[  +0.000104] lpc_ich: Resource conflict(s) found affecting gpio_ich
[  +0.003735] i2c i2c-0: Systems with more than 4 memory slots not supported yet, not instantiating SPD
[  +7.892055] systemd[1]: Configuration file /lib/systemd/system/kubelet.service is marked world-inaccessible. This has no effect as configuration data is accessible via APIs without restrictions. Proceeding anyway.
[ +11.057066] prl_tg: loading out-of-tree module taints kernel.
[  +0.000005] prl_tg: module license 'Parallels' taints kernel.
[  +0.000000] Disabling lock debugging due to kernel taint
[  +1.224482] thermal thermal_zone0: failed to read out thermal zone (-22)
[  +1.728611] virtio_gpu virtio2: [drm] drm_plane_enable_fb_damage_clips() not called
[Nov 1 13:43] kauditd_printk_skb: 79 callbacks suppressed
[Nov 1 13:50] kauditd_printk_skb: 19 callbacks suppressed
[ +18.139355] kauditd_printk_skb: 21 callbacks suppressed
[ +24.204758] kauditd_printk_skb: 25 callbacks suppressed

* 
* ==> etcd [259c449a294f] <==
* {"level":"info","ts":"2022-11-01T13:53:23.495Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 6"}
{"level":"info","ts":"2022-11-01T13:53:23.495Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 6"}
{"level":"info","ts":"2022-11-01T13:53:23.498Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-11-01T13:53:23.498Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-11-01T13:53:23.621Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-11-01T13:53:23.621Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-11-01T13:53:23.498Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-11-01T13:53:23.647Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}
{"level":"info","ts":"2022-11-01T13:53:23.647Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2022-11-01T13:53:30.680Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"103.101747ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016784085683363 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/storage-provisioner\" mod_revision:27423 > success:<request_put:<key:\"/registry/pods/kube-system/storage-provisioner\" value_size:4042 >> failure:<request_range:<key:\"/registry/pods/kube-system/storage-provisioner\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:53:30.681Z","caller":"traceutil/trace.go:171","msg":"trace[1932813712] transaction","detail":"{read_only:false; response_revision:115192; number_of_response:1; }","duration":"112.855408ms","start":"2022-11-01T13:53:30.568Z","end":"2022-11-01T13:53:30.681Z","steps":["trace[1932813712] 'compare'  (duration: 102.723148ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:31.298Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"132.369759ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:722"}
{"level":"info","ts":"2022-11-01T13:53:31.298Z","caller":"traceutil/trace.go:171","msg":"trace[813605144] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:115195; }","duration":"132.456648ms","start":"2022-11-01T13:53:31.166Z","end":"2022-11-01T13:53:31.298Z","steps":["trace[813605144] 'range keys from in-memory index tree'  (duration: 132.123702ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:31.298Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"128.435106ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/\" range_end:\"/registry/storageclasses0\" ","response":"range_response_count:1 size:993"}
{"level":"info","ts":"2022-11-01T13:53:31.298Z","caller":"traceutil/trace.go:171","msg":"trace[1373572528] range","detail":"{range_begin:/registry/storageclasses/; range_end:/registry/storageclasses0; response_count:1; response_revision:115195; }","duration":"128.71837ms","start":"2022-11-01T13:53:31.170Z","end":"2022-11-01T13:53:31.298Z","steps":["trace[1373572528] 'range keys from in-memory index tree'  (duration: 128.192313ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:31.528Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"129.643831ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/standard\" ","response":"range_response_count:1 size:993"}
{"level":"info","ts":"2022-11-01T13:53:31.528Z","caller":"traceutil/trace.go:171","msg":"trace[1954288265] range","detail":"{range_begin:/registry/storageclasses/standard; range_end:; response_count:1; response_revision:115196; }","duration":"129.762816ms","start":"2022-11-01T13:53:31.398Z","end":"2022-11-01T13:53:31.528Z","steps":["trace[1954288265] 'range keys from in-memory index tree'  (duration: 129.498917ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:32.063Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"169.614813ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016784085683384 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-controller-manager-minikube.17237a5298018864\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/kube-controller-manager-minikube.17237a5298018864\" value_size:589 lease:8128016784085683070 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:53:32.064Z","caller":"traceutil/trace.go:171","msg":"trace[464217319] transaction","detail":"{read_only:false; response_revision:115200; number_of_response:1; }","duration":"170.140314ms","start":"2022-11-01T13:53:31.893Z","end":"2022-11-01T13:53:32.064Z","steps":["trace[464217319] 'compare'  (duration: 169.512521ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:33.423Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"100.078839ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:53:33.423Z","caller":"traceutil/trace.go:171","msg":"trace[1442651142] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:115210; }","duration":"100.283266ms","start":"2022-11-01T13:53:33.323Z","end":"2022-11-01T13:53:33.423Z","steps":["trace[1442651142] 'agreement among raft nodes before linearized reading'  (duration: 35.680345ms)","trace[1442651142] 'range keys from in-memory index tree'  (duration: 64.330838ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:53:34.554Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"184.249567ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016784085683421 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/etcd-minikube\" mod_revision:27329 > success:<request_put:<key:\"/registry/pods/kube-system/etcd-minikube\" value_size:5370 >> failure:<request_range:<key:\"/registry/pods/kube-system/etcd-minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:53:34.555Z","caller":"traceutil/trace.go:171","msg":"trace[120826367] transaction","detail":"{read_only:false; response_revision:115217; number_of_response:1; }","duration":"186.242218ms","start":"2022-11-01T13:53:34.369Z","end":"2022-11-01T13:53:34.555Z","steps":["trace[120826367] 'compare'  (duration: 183.511244ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:38.734Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"165.741067ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016784085683470 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/coredns-565d847f94-r4f4l\" mod_revision:115222 > success:<request_put:<key:\"/registry/pods/kube-system/coredns-565d847f94-r4f4l\" value_size:4833 >> failure:<request_range:<key:\"/registry/pods/kube-system/coredns-565d847f94-r4f4l\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:53:38.734Z","caller":"traceutil/trace.go:171","msg":"trace[1818435907] transaction","detail":"{read_only:false; response_revision:115247; number_of_response:1; }","duration":"166.050295ms","start":"2022-11-01T13:53:38.568Z","end":"2022-11-01T13:53:38.734Z","steps":["trace[1818435907] 'compare'  (duration: 165.626277ms)"],"step_count":1}
{"level":"info","ts":"2022-11-01T13:53:39.390Z","caller":"traceutil/trace.go:171","msg":"trace[359256410] transaction","detail":"{read_only:false; response_revision:115252; number_of_response:1; }","duration":"124.375066ms","start":"2022-11-01T13:53:39.266Z","end":"2022-11-01T13:53:39.390Z","steps":["trace[359256410] 'process raft request'  (duration: 72.824445ms)","trace[359256410] 'compare'  (duration: 51.30673ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:53:49.396Z","caller":"traceutil/trace.go:171","msg":"trace[745920562] transaction","detail":"{read_only:false; response_revision:115289; number_of_response:1; }","duration":"130.299497ms","start":"2022-11-01T13:53:49.266Z","end":"2022-11-01T13:53:49.396Z","steps":["trace[745920562] 'process raft request'  (duration: 69.652789ms)","trace[745920562] 'compare'  (duration: 59.629389ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:53:50.576Z","caller":"traceutil/trace.go:171","msg":"trace[1219705103] linearizableReadLoop","detail":"{readStateIndex:148489; appliedIndex:148489; }","duration":"130.891593ms","start":"2022-11-01T13:53:50.445Z","end":"2022-11-01T13:53:50.576Z","steps":["trace[1219705103] 'read index received'  (duration: 130.882519ms)","trace[1219705103] 'applied index is now lower than readState.Index'  (duration: 7.863µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:53:50.643Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"198.388146ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/mysql8-66649767dd-rkzjt\" ","response":"range_response_count:1 size:3592"}
{"level":"info","ts":"2022-11-01T13:53:50.643Z","caller":"traceutil/trace.go:171","msg":"trace[955749638] range","detail":"{range_begin:/registry/pods/default/mysql8-66649767dd-rkzjt; range_end:; response_count:1; response_revision:115296; }","duration":"198.516381ms","start":"2022-11-01T13:53:50.445Z","end":"2022-11-01T13:53:50.643Z","steps":["trace[955749638] 'agreement among raft nodes before linearized reading'  (duration: 131.044393ms)","trace[955749638] 'range keys from in-memory index tree'  (duration: 67.300468ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:53:50.643Z","caller":"traceutil/trace.go:171","msg":"trace[1798513936] transaction","detail":"{read_only:false; response_revision:115297; number_of_response:1; }","duration":"195.093667ms","start":"2022-11-01T13:53:50.448Z","end":"2022-11-01T13:53:50.643Z","steps":["trace[1798513936] 'process raft request'  (duration: 127.479086ms)","trace[1798513936] 'compare'  (duration: 67.545729ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:53:50.705Z","caller":"traceutil/trace.go:171","msg":"trace[2069947769] linearizableReadLoop","detail":"{readStateIndex:148490; appliedIndex:148490; }","duration":"128.937362ms","start":"2022-11-01T13:53:50.576Z","end":"2022-11-01T13:53:50.705Z","steps":["trace[2069947769] 'read index received'  (duration: 128.928384ms)","trace[2069947769] 'applied index is now lower than readState.Index'  (duration: 8.142µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:53:50.705Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"232.708415ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/default/mysql-pvc\" ","response":"range_response_count:1 size:1299"}
{"level":"info","ts":"2022-11-01T13:53:50.705Z","caller":"traceutil/trace.go:171","msg":"trace[2118965866] range","detail":"{range_begin:/registry/persistentvolumeclaims/default/mysql-pvc; range_end:; response_count:1; response_revision:115297; }","duration":"232.745961ms","start":"2022-11-01T13:53:50.472Z","end":"2022-11-01T13:53:50.705Z","steps":["trace[2118965866] 'agreement among raft nodes before linearized reading'  (duration: 232.670048ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:53:50.840Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"131.130533ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumes/mysql-pv\" ","response":"range_response_count:1 size:1529"}
{"level":"info","ts":"2022-11-01T13:53:50.840Z","caller":"traceutil/trace.go:171","msg":"trace[1050000590] range","detail":"{range_begin:/registry/persistentvolumes/mysql-pv; range_end:; response_count:1; response_revision:115298; }","duration":"131.192136ms","start":"2022-11-01T13:53:50.709Z","end":"2022-11-01T13:53:50.840Z","steps":["trace[1050000590] 'agreement among raft nodes before linearized reading'  (duration: 49.476722ms)","trace[1050000590] 'range keys from in-memory index tree'  (duration: 81.556312ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:53:50.841Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"131.830694ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/storage-provisioner\" ","response":"range_response_count:1 size:4216"}
{"level":"info","ts":"2022-11-01T13:53:50.841Z","caller":"traceutil/trace.go:171","msg":"trace[809410988] range","detail":"{range_begin:/registry/pods/kube-system/storage-provisioner; range_end:; response_count:1; response_revision:115301; }","duration":"131.895857ms","start":"2022-11-01T13:53:50.709Z","end":"2022-11-01T13:53:50.841Z","steps":["trace[809410988] 'agreement among raft nodes before linearized reading'  (duration: 131.797652ms)"],"step_count":1}
{"level":"info","ts":"2022-11-01T13:53:50.841Z","caller":"traceutil/trace.go:171","msg":"trace[1820283416] transaction","detail":"{read_only:false; response_revision:115300; number_of_response:1; }","duration":"131.502548ms","start":"2022-11-01T13:53:50.710Z","end":"2022-11-01T13:53:50.841Z","steps":["trace[1820283416] 'process raft request'  (duration: 131.193861ms)"],"step_count":1}
{"level":"info","ts":"2022-11-01T13:53:50.841Z","caller":"traceutil/trace.go:171","msg":"trace[183060032] transaction","detail":"{read_only:false; response_revision:115299; number_of_response:1; }","duration":"131.884886ms","start":"2022-11-01T13:53:50.710Z","end":"2022-11-01T13:53:50.841Z","steps":["trace[183060032] 'process raft request'  (duration: 48.922737ms)","trace[183060032] 'compare'  (duration: 82.208707ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:53:50.842Z","caller":"traceutil/trace.go:171","msg":"trace[1802552074] transaction","detail":"{read_only:false; response_revision:115301; number_of_response:1; }","duration":"131.440602ms","start":"2022-11-01T13:53:50.710Z","end":"2022-11-01T13:53:50.842Z","steps":["trace[1802552074] 'process raft request'  (duration: 130.614451ms)"],"step_count":1}
{"level":"info","ts":"2022-11-01T13:53:59.382Z","caller":"traceutil/trace.go:171","msg":"trace[1416311056] transaction","detail":"{read_only:false; response_revision:115306; number_of_response:1; }","duration":"116.540917ms","start":"2022-11-01T13:53:59.266Z","end":"2022-11-01T13:53:59.382Z","steps":["trace[1416311056] 'process raft request'  (duration: 73.500713ms)","trace[1416311056] 'compare'  (duration: 42.92348ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:55:39.819Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"335.418171ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016784085684108 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:115340 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128016784085684106 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:55:39.820Z","caller":"traceutil/trace.go:171","msg":"trace[309601509] linearizableReadLoop","detail":"{readStateIndex:148564; appliedIndex:148563; }","duration":"388.842349ms","start":"2022-11-01T13:55:39.431Z","end":"2022-11-01T13:55:39.820Z","steps":["trace[309601509] 'read index received'  (duration: 53.024824ms)","trace[309601509] 'applied index is now lower than readState.Index'  (duration: 335.816601ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:55:39.820Z","caller":"traceutil/trace.go:171","msg":"trace[1375172863] transaction","detail":"{read_only:false; response_revision:115347; number_of_response:1; }","duration":"424.11251ms","start":"2022-11-01T13:55:39.396Z","end":"2022-11-01T13:55:39.820Z","steps":["trace[1375172863] 'process raft request'  (duration: 88.337574ms)","trace[1375172863] 'compare'  (duration: 335.285775ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:55:39.820Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:55:39.396Z","time spent":"424.183241ms","remote":"127.0.0.1:48090","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:115340 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128016784085684106 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2022-11-01T13:55:39.820Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"389.009906ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:55:39.820Z","caller":"traceutil/trace.go:171","msg":"trace[1725865884] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115347; }","duration":"389.230963ms","start":"2022-11-01T13:55:39.431Z","end":"2022-11-01T13:55:39.820Z","steps":["trace[1725865884] 'agreement among raft nodes before linearized reading'  (duration: 388.970187ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:55:39.820Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:55:39.431Z","time spent":"389.282542ms","remote":"127.0.0.1:48044","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-01T13:55:40.090Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"115.053989ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2022-11-01T13:55:40.090Z","caller":"traceutil/trace.go:171","msg":"trace[2093516084] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:115347; }","duration":"115.139905ms","start":"2022-11-01T13:55:39.975Z","end":"2022-11-01T13:55:40.090Z","steps":["trace[2093516084] 'range keys from in-memory index tree'  (duration: 114.932512ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:55:41.636Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"207.985672ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:55:41.636Z","caller":"traceutil/trace.go:171","msg":"trace[1424895279] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115349; }","duration":"208.050866ms","start":"2022-11-01T13:55:41.428Z","end":"2022-11-01T13:55:41.636Z","steps":["trace[1424895279] 'range keys from in-memory index tree'  (duration: 207.858657ms)"],"step_count":1}
{"level":"info","ts":"2022-11-01T13:56:20.537Z","caller":"traceutil/trace.go:171","msg":"trace[325251915] linearizableReadLoop","detail":"{readStateIndex:148601; appliedIndex:148601; }","duration":"110.245849ms","start":"2022-11-01T13:56:20.427Z","end":"2022-11-01T13:56:20.537Z","steps":["trace[325251915] 'read index received'  (duration: 110.240679ms)","trace[325251915] 'applied index is now lower than readState.Index'  (duration: 4.182µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:56:20.789Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"362.325062ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:56:20.790Z","caller":"traceutil/trace.go:171","msg":"trace[675211905] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115376; }","duration":"362.454598ms","start":"2022-11-01T13:56:20.427Z","end":"2022-11-01T13:56:20.790Z","steps":["trace[675211905] 'agreement among raft nodes before linearized reading'  (duration: 110.345416ms)","trace[675211905] 'range keys from in-memory index tree'  (duration: 251.946855ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:56:20.790Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:56:20.427Z","time spent":"362.506661ms","remote":"127.0.0.1:48044","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2022-11-01T13:56:36.530Z","caller":"traceutil/trace.go:171","msg":"trace[387876523] linearizableReadLoop","detail":"{readStateIndex:148615; appliedIndex:148615; }","duration":"103.780619ms","start":"2022-11-01T13:56:36.427Z","end":"2022-11-01T13:56:36.530Z","steps":["trace[387876523] 'read index received'  (duration: 103.774201ms)","trace[387876523] 'applied index is now lower than readState.Index'  (duration: 5.593µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:56:36.531Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"104.349442ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:56:36.531Z","caller":"traceutil/trace.go:171","msg":"trace[1989778777] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115387; }","duration":"104.422256ms","start":"2022-11-01T13:56:36.427Z","end":"2022-11-01T13:56:36.531Z","steps":["trace[1989778777] 'agreement among raft nodes before linearized reading'  (duration: 103.862849ms)"],"step_count":1}

* 
* ==> etcd [87e47d44e685] <==
* {"level":"warn","ts":"2022-11-01T13:40:38.176Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"846.684078ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:38.176Z","caller":"traceutil/trace.go:171","msg":"trace[18173233] range","detail":"{range_begin:/registry/ingress/; range_end:/registry/ingress0; response_count:0; response_revision:115150; }","duration":"846.7485ms","start":"2022-11-01T13:40:37.329Z","end":"2022-11-01T13:40:38.176Z","steps":["trace[18173233] 'agreement among raft nodes before linearized reading'  (duration: 175.191161ms)","trace[18173233] 'count revisions from in-memory index tree'  (duration: 671.47752ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:38.176Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:37.329Z","time spent":"846.79441ms","remote":"127.0.0.1:40190","response type":"/etcdserverpb.KV/Range","request count":0,"request size":42,"response count":0,"response size":30,"request content":"key:\"/registry/ingress/\" range_end:\"/registry/ingress0\" count_only:true "}
{"level":"warn","ts":"2022-11-01T13:40:38.177Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.513367927s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:38.177Z","caller":"traceutil/trace.go:171","msg":"trace[1394271815] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115150; }","duration":"1.514059435s","start":"2022-11-01T13:40:36.663Z","end":"2022-11-01T13:40:38.177Z","steps":["trace[1394271815] 'agreement among raft nodes before linearized reading'  (duration: 841.653492ms)","trace[1394271815] 'range keys from in-memory index tree'  (duration: 671.70935ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:38.177Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:36.663Z","time spent":"1.514096618s","remote":"127.0.0.1:40412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-01T13:40:38.177Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"389.097881ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2022-11-01T13:40:38.177Z","caller":"traceutil/trace.go:171","msg":"trace[548537113] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:115150; }","duration":"389.314922ms","start":"2022-11-01T13:40:37.788Z","end":"2022-11-01T13:40:38.177Z","steps":["trace[548537113] 'range keys from in-memory index tree'  (duration: 389.035898ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:38.178Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:37.788Z","time spent":"389.394245ms","remote":"127.0.0.1:40014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1137,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"info","ts":"2022-11-01T13:40:38.466Z","caller":"traceutil/trace.go:171","msg":"trace[2130101464] linearizableReadLoop","detail":"{readStateIndex:148321; appliedIndex:148321; }","duration":"281.65361ms","start":"2022-11-01T13:40:38.184Z","end":"2022-11-01T13:40:38.466Z","steps":["trace[2130101464] 'read index received'  (duration: 281.643526ms)","trace[2130101464] 'applied index is now lower than readState.Index'  (duration: 7.758µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:39.643Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"679.304835ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2022-11-01T13:40:39.643Z","caller":"traceutil/trace.go:171","msg":"trace[1007561656] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:115151; }","duration":"679.520254ms","start":"2022-11-01T13:40:38.964Z","end":"2022-11-01T13:40:39.643Z","steps":["trace[1007561656] 'count revisions from in-memory index tree'  (duration: 679.222258ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:39.643Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"649.88032ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016661856042800 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:115145 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:40:39.644Z","caller":"traceutil/trace.go:171","msg":"trace[1322089046] transaction","detail":"{read_only:false; response_revision:115152; number_of_response:1; }","duration":"650.211275ms","start":"2022-11-01T13:40:38.993Z","end":"2022-11-01T13:40:39.644Z","steps":["trace[1322089046] 'compare'  (duration: 649.811015ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:39.644Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:38.993Z","time spent":"650.307573ms","remote":"127.0.0.1:40148","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:115145 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2022-11-01T13:40:39.643Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:38.964Z","time spent":"679.764749ms","remote":"127.0.0.1:40436","response type":"/etcdserverpb.KV/Range","request count":0,"request size":40,"response count":60,"response size":32,"request content":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true "}
{"level":"warn","ts":"2022-11-01T13:40:39.643Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.459036971s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:39.644Z","caller":"traceutil/trace.go:171","msg":"trace[1717623435] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115151; }","duration":"1.459938515s","start":"2022-11-01T13:40:38.184Z","end":"2022-11-01T13:40:39.644Z","steps":["trace[1717623435] 'agreement among raft nodes before linearized reading'  (duration: 281.865563ms)","trace[1717623435] 'range keys from in-memory index tree'  (duration: 1.177166026s)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:39.644Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:38.184Z","time spent":"1.459989788s","remote":"127.0.0.1:40412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2022-11-01T13:40:39.643Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"1.228863819s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/default\" ","response":"range_response_count:1 size:342"}
{"level":"info","ts":"2022-11-01T13:40:39.644Z","caller":"traceutil/trace.go:171","msg":"trace[1363565747] range","detail":"{range_begin:/registry/namespaces/default; range_end:; response_count:1; response_revision:115151; }","duration":"1.230026222s","start":"2022-11-01T13:40:38.414Z","end":"2022-11-01T13:40:39.644Z","steps":["trace[1363565747] 'agreement among raft nodes before linearized reading'  (duration: 51.694417ms)","trace[1363565747] 'range keys from in-memory index tree'  (duration: 1.17712296s)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:39.645Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:38.414Z","time spent":"1.230071611s","remote":"127.0.0.1:39998","response type":"/etcdserverpb.KV/Range","request count":0,"request size":30,"response count":1,"response size":366,"request content":"key:\"/registry/namespaces/default\" "}
{"level":"warn","ts":"2022-11-01T13:40:40.020Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"212.840717ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016661856042807 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:115148 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128016661856042805 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:40:40.020Z","caller":"traceutil/trace.go:171","msg":"trace[588490925] linearizableReadLoop","detail":"{readStateIndex:148324; appliedIndex:148323; }","duration":"368.470818ms","start":"2022-11-01T13:40:39.652Z","end":"2022-11-01T13:40:40.020Z","steps":["trace[588490925] 'read index received'  (duration: 155.479851ms)","trace[588490925] 'applied index is now lower than readState.Index'  (duration: 212.990247ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:40.020Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"368.639125ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:40.020Z","caller":"traceutil/trace.go:171","msg":"trace[1953428566] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115153; }","duration":"368.76295ms","start":"2022-11-01T13:40:39.652Z","end":"2022-11-01T13:40:40.020Z","steps":["trace[1953428566] 'agreement among raft nodes before linearized reading'  (duration: 368.572478ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:40.020Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:39.652Z","time spent":"368.812477ms","remote":"127.0.0.1:40412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2022-11-01T13:40:40.021Z","caller":"traceutil/trace.go:171","msg":"trace[2047520902] transaction","detail":"{read_only:false; response_revision:115153; number_of_response:1; }","duration":"371.339176ms","start":"2022-11-01T13:40:39.649Z","end":"2022-11-01T13:40:40.020Z","steps":["trace[2047520902] 'process raft request'  (duration: 157.940625ms)","trace[2047520902] 'compare'  (duration: 212.759133ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:40.021Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:39.649Z","time spent":"371.475107ms","remote":"127.0.0.1:39914","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:115148 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128016661856042805 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2022-11-01T13:40:40.390Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"205.067531ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2022-11-01T13:40:40.390Z","caller":"traceutil/trace.go:171","msg":"trace[277969692] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:115153; }","duration":"205.166494ms","start":"2022-11-01T13:40:40.185Z","end":"2022-11-01T13:40:40.390Z","steps":["trace[277969692] 'range keys from in-memory index tree'  (duration: 204.984112ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:40.830Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"162.945676ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:40.830Z","caller":"traceutil/trace.go:171","msg":"trace[695033143] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115154; }","duration":"163.139485ms","start":"2022-11-01T13:40:40.667Z","end":"2022-11-01T13:40:40.830Z","steps":["trace[695033143] 'agreement among raft nodes before linearized reading'  (duration: 28.507489ms)","trace[695033143] 'range keys from in-memory index tree'  (duration: 134.421232ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:42.853Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"160.818536ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:42.853Z","caller":"traceutil/trace.go:171","msg":"trace[88775926] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115155; }","duration":"160.927039ms","start":"2022-11-01T13:40:42.693Z","end":"2022-11-01T13:40:42.853Z","steps":["trace[88775926] 'range keys from in-memory index tree'  (duration: 160.767179ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:49.169Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"684.917109ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:421"}
{"level":"info","ts":"2022-11-01T13:40:49.169Z","caller":"traceutil/trace.go:171","msg":"trace[1380943271] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:115158; }","duration":"684.973489ms","start":"2022-11-01T13:40:48.484Z","end":"2022-11-01T13:40:49.169Z","steps":["trace[1380943271] 'agreement among raft nodes before linearized reading'  (duration: 41.214043ms)","trace[1380943271] 'range keys from in-memory index tree'  (duration: 643.68078ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:49.169Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:48.484Z","time spent":"685.007728ms","remote":"127.0.0.1:40014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":445,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"warn","ts":"2022-11-01T13:40:49.169Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"643.793217ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128016661856042852 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:115157 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2022-11-01T13:40:49.169Z","caller":"traceutil/trace.go:171","msg":"trace[2105042916] linearizableReadLoop","detail":"{readStateIndex:148332; appliedIndex:148331; }","duration":"643.911882ms","start":"2022-11-01T13:40:48.525Z","end":"2022-11-01T13:40:49.169Z","steps":["trace[2105042916] 'read index received'  (duration: 396.067472ms)","trace[2105042916] 'applied index is now lower than readState.Index'  (duration: 247.843796ms)"],"step_count":2}
{"level":"info","ts":"2022-11-01T13:40:49.169Z","caller":"traceutil/trace.go:171","msg":"trace[402594534] transaction","detail":"{read_only:false; response_revision:115159; number_of_response:1; }","duration":"683.975052ms","start":"2022-11-01T13:40:48.485Z","end":"2022-11-01T13:40:49.169Z","steps":["trace[402594534] 'process raft request'  (duration: 39.929012ms)","trace[402594534] 'compare'  (duration: 643.706754ms)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:49.169Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:48.485Z","time spent":"684.535892ms","remote":"127.0.0.1:40014","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:115157 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2022-11-01T13:40:49.169Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"111.00538ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2022-11-01T13:40:49.169Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"679.656156ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2022-11-01T13:40:49.170Z","caller":"traceutil/trace.go:171","msg":"trace[768524377] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:115159; }","duration":"111.080041ms","start":"2022-11-01T13:40:49.058Z","end":"2022-11-01T13:40:49.170Z","steps":["trace[768524377] 'agreement among raft nodes before linearized reading'  (duration: 110.995431ms)"],"step_count":1}
{"level":"info","ts":"2022-11-01T13:40:49.170Z","caller":"traceutil/trace.go:171","msg":"trace[103099727] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:115159; }","duration":"680.160093ms","start":"2022-11-01T13:40:48.489Z","end":"2022-11-01T13:40:49.170Z","steps":["trace[103099727] 'agreement among raft nodes before linearized reading'  (duration: 679.632802ms)"],"step_count":1}
{"level":"warn","ts":"2022-11-01T13:40:49.170Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:48.489Z","time spent":"680.328488ms","remote":"127.0.0.1:40412","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2022-11-01T13:40:52.207Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2022-11-01T13:40:53.862Z","caller":"embed/etcd.go:368","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-11-01T13:40:53.946Z","caller":"traceutil/trace.go:171","msg":"trace[1252396149] linearizableReadLoop","detail":"{readStateIndex:148335; appliedIndex:148335; }","duration":"237.040417ms","start":"2022-11-01T13:40:53.709Z","end":"2022-11-01T13:40:53.946Z","steps":["trace[1252396149] 'read index received'  (duration: 237.035144ms)","trace[1252396149] 'applied index is now lower than readState.Index'  (duration: 4.866µs)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:57.047Z","caller":"etcdserver/util.go:166","msg":"apply request took too long","took":"3.338034045s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"","error":"context canceled"}
{"level":"info","ts":"2022-11-01T13:40:57.047Z","caller":"traceutil/trace.go:171","msg":"trace[1668141535] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; }","duration":"3.338077181s","start":"2022-11-01T13:40:53.709Z","end":"2022-11-01T13:40:57.047Z","steps":["trace[1668141535] 'agreement among raft nodes before linearized reading'  (duration: 237.097841ms)","trace[1668141535] 'range keys from in-memory index tree'  (duration: 3.100924238s)"],"step_count":2}
{"level":"warn","ts":"2022-11-01T13:40:57.047Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2022-11-01T13:40:53.709Z","time spent":"3.338105491s","remote":"127.0.0.1:40014","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":0,"response size":0,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
WARNING: 2022/11/01 13:40:57 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
WARNING: 2022/11/01 13:41:02 [core] grpc: addrConn.createTransport failed to connect to {127.0.0.1:2379 127.0.0.1:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused". Reconnecting...
WARNING: 2022/11/01 13:41:02 [core] grpc: addrConn.createTransport failed to connect to {192.168.49.2:2379 192.168.49.2:2379 <nil> 0 <nil>}. Err: connection error: desc = "transport: Error while dialing dial tcp 192.168.49.2:2379: connect: connection refused". Reconnecting...
{"level":"info","ts":"2022-11-01T13:41:02.857Z","caller":"etcdserver/server.go:1453","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2022-11-01T13:41:04.125Z","caller":"embed/etcd.go:563","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-11-01T13:41:04.126Z","caller":"embed/etcd.go:568","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-11-01T13:41:04.126Z","caller":"embed/etcd.go:370","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  13:56:50 up 14 min,  0 users,  load average: 3.18, 3.80, 2.60
Linux minikube 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.5 LTS"

* 
* ==> kube-apiserver [7f99ff807be6] <==
* W1101 13:53:25.400196       1 genericapiserver.go:656] Skipping API policy/v1beta1 because it has no resources.
W1101 13:53:25.411973       1 genericapiserver.go:656] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W1101 13:53:25.412027       1 genericapiserver.go:656] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
W1101 13:53:25.416087       1 genericapiserver.go:656] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W1101 13:53:25.416140       1 genericapiserver.go:656] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
W1101 13:53:25.430584       1 genericapiserver.go:656] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
W1101 13:53:25.455004       1 genericapiserver.go:656] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W1101 13:53:25.480144       1 genericapiserver.go:656] Skipping API apps/v1beta2 because it has no resources.
W1101 13:53:25.480207       1 genericapiserver.go:656] Skipping API apps/v1beta1 because it has no resources.
W1101 13:53:25.485977       1 genericapiserver.go:656] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W1101 13:53:25.490764       1 genericapiserver.go:656] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1101 13:53:25.492432       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1101 13:53:25.492506       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1101 13:53:25.520510       1 genericapiserver.go:656] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1101 13:53:27.826145       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1101 13:53:27.829969       1 secure_serving.go:210] Serving securely on [::]:8443
I1101 13:53:27.830430       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1101 13:53:27.831819       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1101 13:53:27.832212       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1101 13:53:27.836361       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1101 13:53:27.836530       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1101 13:53:27.839867       1 apf_controller.go:300] Starting API Priority and Fairness config controller
I1101 13:53:27.841019       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1101 13:53:27.893102       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1101 13:53:27.893184       1 autoregister_controller.go:141] Starting autoregister controller
I1101 13:53:27.893194       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1101 13:53:27.893240       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1101 13:53:27.897056       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1101 13:53:27.897167       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1101 13:53:27.897271       1 controller.go:83] Starting OpenAPI AggregationController
I1101 13:53:27.920320       1 available_controller.go:491] Starting AvailableConditionController
I1101 13:53:27.920370       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1101 13:53:27.938534       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1101 13:53:27.938693       1 controller.go:85] Starting OpenAPI controller
I1101 13:53:27.938768       1 controller.go:85] Starting OpenAPI V3 controller
I1101 13:53:27.939226       1 naming_controller.go:291] Starting NamingConditionController
I1101 13:53:27.939257       1 establishing_controller.go:76] Starting EstablishingController
I1101 13:53:27.939280       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1101 13:53:27.939303       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1101 13:53:27.939320       1 crd_finalizer.go:266] Starting CRDFinalizer
I1101 13:53:27.939349       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1101 13:53:27.939406       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1101 13:53:27.950420       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1101 13:53:28.197274       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1101 13:53:28.201856       1 cache.go:39] Caches are synced for autoregister controller
I1101 13:53:28.227583       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1101 13:53:28.228892       1 controller.go:616] quota admission added evaluator for: leases.coordination.k8s.io
I1101 13:53:28.236915       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1101 13:53:28.240206       1 apf_controller.go:305] Running API Priority and Fairness config worker
I1101 13:53:28.276926       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1101 13:53:28.277222       1 shared_informer.go:262] Caches are synced for node_authorizer
I1101 13:53:28.326838       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1101 13:53:28.844417       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1101 13:53:30.179901       1 controller.go:616] quota admission added evaluator for: serviceaccounts
I1101 13:53:30.234839       1 controller.go:616] quota admission added evaluator for: deployments.apps
I1101 13:53:30.310709       1 controller.go:616] quota admission added evaluator for: daemonsets.apps
I1101 13:53:30.358073       1 controller.go:616] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1101 13:53:30.372583       1 controller.go:616] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1101 13:53:40.979390       1 controller.go:616] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1101 13:53:40.986129       1 controller.go:616] quota admission added evaluator for: endpoints

* 
* ==> kube-apiserver [c04a84748727] <==
*   "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1101 13:41:03.397485       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1101 13:41:03.397570       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1101 13:41:03.397640       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
I1101 13:41:03.397654       1 storage_flowcontrol.go:172] APF bootstrap ensurer is exiting
W1101 13:41:03.397720       1 logging.go:59] [core] [Channel #13 SubChannel #14] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1101 13:41:02.929701       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
I1101 13:41:03.485916       1 apf_controller.go:309] Shutting down API Priority and Fairness config worker
W1101 13:41:03.486033       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W1101 13:41:03.486083       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [5460ec2ef5e9] <==
* I1101 13:53:40.600490       1 attach_detach_controller.go:328] Starting attach detach controller
I1101 13:53:40.600512       1 shared_informer.go:255] Waiting for caches to sync for attach detach
I1101 13:53:40.604024       1 controllermanager.go:602] Started "ttl-after-finished"
I1101 13:53:40.604049       1 ttlafterfinished_controller.go:109] Starting TTL after finished controller
I1101 13:53:40.604722       1 shared_informer.go:255] Waiting for caches to sync for TTL after finished
I1101 13:53:40.622707       1 controllermanager.go:602] Started "namespace"
I1101 13:53:40.622902       1 namespace_controller.go:200] Starting namespace controller
I1101 13:53:40.622921       1 shared_informer.go:255] Waiting for caches to sync for namespace
I1101 13:53:40.625688       1 controllermanager.go:602] Started "csrcleaner"
I1101 13:53:40.625896       1 cleaner.go:82] Starting CSR cleaner controller
I1101 13:53:40.636959       1 shared_informer.go:255] Waiting for caches to sync for resource quota
W1101 13:53:40.649335       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1101 13:53:40.724964       1 shared_informer.go:262] Caches are synced for PV protection
I1101 13:53:40.726404       1 shared_informer.go:262] Caches are synced for TTL
I1101 13:53:40.749259       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I1101 13:53:40.749330       1 shared_informer.go:262] Caches are synced for expand
I1101 13:53:40.749662       1 shared_informer.go:262] Caches are synced for namespace
I1101 13:53:40.760899       1 shared_informer.go:262] Caches are synced for TTL after finished
I1101 13:53:40.809817       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I1101 13:53:40.810234       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1101 13:53:40.810666       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I1101 13:53:40.810875       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I1101 13:53:40.812221       1 shared_informer.go:262] Caches are synced for crt configmap
I1101 13:53:40.815012       1 shared_informer.go:262] Caches are synced for node
I1101 13:53:40.815172       1 range_allocator.go:166] Starting range CIDR allocator
I1101 13:53:40.815180       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I1101 13:53:40.815195       1 shared_informer.go:262] Caches are synced for cidrallocator
I1101 13:53:40.815580       1 shared_informer.go:262] Caches are synced for service account
I1101 13:53:40.816711       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I1101 13:53:40.817997       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1101 13:53:40.823961       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I1101 13:53:40.834174       1 shared_informer.go:262] Caches are synced for cronjob
I1101 13:53:40.834292       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I1101 13:53:40.866479       1 shared_informer.go:262] Caches are synced for disruption
I1101 13:53:40.866608       1 shared_informer.go:262] Caches are synced for taint
I1101 13:53:40.866700       1 taint_manager.go:204] "Starting NoExecuteTaintManager"
I1101 13:53:40.866768       1 taint_manager.go:209] "Sending events to api server"
I1101 13:53:40.866770       1 node_lifecycle_controller.go:1443] Initializing eviction metric for zone: 
W1101 13:53:40.866950       1 node_lifecycle_controller.go:1058] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1101 13:53:40.867012       1 node_lifecycle_controller.go:1259] Controller detected that zone  is now in state Normal.
I1101 13:53:40.867096       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1101 13:53:40.868829       1 shared_informer.go:262] Caches are synced for persistent volume
I1101 13:53:40.875528       1 shared_informer.go:262] Caches are synced for PVC protection
I1101 13:53:40.878093       1 shared_informer.go:262] Caches are synced for ephemeral
I1101 13:53:40.887982       1 shared_informer.go:262] Caches are synced for resource quota
I1101 13:53:40.895478       1 shared_informer.go:262] Caches are synced for job
I1101 13:53:40.901349       1 shared_informer.go:262] Caches are synced for attach detach
I1101 13:53:40.917825       1 shared_informer.go:262] Caches are synced for daemon sets
I1101 13:53:40.921950       1 shared_informer.go:262] Caches are synced for GC
I1101 13:53:40.922033       1 shared_informer.go:262] Caches are synced for HPA
I1101 13:53:40.922050       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1101 13:53:40.925691       1 shared_informer.go:262] Caches are synced for deployment
I1101 13:53:40.951604       1 shared_informer.go:262] Caches are synced for endpoint
I1101 13:53:40.951730       1 shared_informer.go:262] Caches are synced for ReplicationController
I1101 13:53:40.951755       1 shared_informer.go:262] Caches are synced for ReplicaSet
I1101 13:53:40.969290       1 shared_informer.go:262] Caches are synced for stateful set
I1101 13:53:40.969433       1 shared_informer.go:262] Caches are synced for resource quota
I1101 13:53:41.284801       1 shared_informer.go:262] Caches are synced for garbage collector
I1101 13:53:41.285150       1 garbagecollector.go:163] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1101 13:53:41.367855       1 shared_informer.go:262] Caches are synced for garbage collector

* 
* ==> kube-controller-manager [af902fca7879] <==
* I1027 15:58:56.098113       1 event.go:294] "Event occurred" object="default/mysql8-5f67dff86b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql8-5f67dff86b-hc4p6"
I1027 16:07:13.369415       1 event.go:294] "Event occurred" object="default/mysql8" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql8-5f8c4fd4f7 to 1"
I1027 16:07:13.402891       1 event.go:294] "Event occurred" object="default/mysql8-5f8c4fd4f7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql8-5f8c4fd4f7-pb7k6"
I1027 16:08:52.146340       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-67b87f8dfc to 1"
I1027 16:08:52.152522       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67b87f8dfc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-67b87f8dfc-nglz4"
I1027 16:40:20.556450       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-67c4fb68c6 to 1"
I1027 16:40:20.622664       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67c4fb68c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-67c4fb68c6-vl8xp"
I1027 16:40:31.000188       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set msvc-usuarios-67b87f8dfc to 0 from 1"
I1027 16:40:31.060409       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67b87f8dfc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: msvc-usuarios-67b87f8dfc-nglz4"
I1027 16:54:18.284729       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-67c4fb68c6 to 3 from 1"
I1027 16:54:18.344903       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67c4fb68c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-67c4fb68c6-94v25"
I1027 16:54:18.383715       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67c4fb68c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-67c4fb68c6-lljfq"
I1027 16:59:26.342750       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set msvc-usuarios-67c4fb68c6 to 1 from 3"
I1027 16:59:27.068565       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67c4fb68c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: msvc-usuarios-67c4fb68c6-lljfq"
I1027 16:59:27.068586       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67c4fb68c6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: msvc-usuarios-67c4fb68c6-vl8xp"
W1027 16:59:27.191149       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/msvc-usuarios", retrying. Error: EndpointSlice informer cache is out of date
I1027 18:01:36.357025       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-67b87f8dfc to 1"
I1027 18:01:36.371894       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67b87f8dfc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-67b87f8dfc-ndr27"
I1027 21:21:35.313696       1 event.go:294] "Event occurred" object="default/postgres14" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres14-6694bfc79 to 1"
I1027 21:21:35.327210       1 event.go:294] "Event occurred" object="default/postgres14-6694bfc79" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres14-6694bfc79-f2f7w"
I1027 21:33:59.463350       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-cursos-56fc646c96 to 1"
I1027 21:33:59.485728       1 event.go:294] "Event occurred" object="default/msvc-cursos-56fc646c96" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-cursos-56fc646c96-647fw"
I1027 21:51:21.633796       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-cursos-7575fcdb75 to 1"
I1027 21:51:21.643503       1 event.go:294] "Event occurred" object="default/msvc-cursos-7575fcdb75" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-cursos-7575fcdb75-v5d2z"
I1027 21:51:26.564688       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set msvc-cursos-56fc646c96 to 0 from 1"
I1027 21:51:26.578792       1 event.go:294] "Event occurred" object="default/msvc-cursos-56fc646c96" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: msvc-cursos-56fc646c96-647fw"
I1027 21:59:24.252131       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-cursos-56fc646c96 to 1 from 0"
I1027 21:59:24.279395       1 event.go:294] "Event occurred" object="default/msvc-cursos-56fc646c96" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-cursos-56fc646c96-bf44j"
I1027 21:59:27.812517       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set msvc-cursos-7575fcdb75 to 0 from 1"
I1027 21:59:27.818935       1 event.go:294] "Event occurred" object="default/msvc-cursos-7575fcdb75" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: msvc-cursos-7575fcdb75-v5d2z"
I1028 08:44:40.493369       1 event.go:294] "Event occurred" object="default/mysql8" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql8-6686564bb5 to 1"
I1028 08:44:40.532351       1 event.go:294] "Event occurred" object="default/mysql8-6686564bb5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql8-6686564bb5-zt9sr"
I1028 08:44:44.199100       1 event.go:294] "Event occurred" object="default/mysql8" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set mysql8-5f8c4fd4f7 to 0 from 1"
I1028 08:44:44.217257       1 event.go:294] "Event occurred" object="default/mysql8-5f8c4fd4f7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: mysql8-5f8c4fd4f7-pb7k6"
I1028 08:47:09.734331       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-67b87f8dfc to 1"
I1028 08:47:09.746576       1 event.go:294] "Event occurred" object="default/msvc-usuarios-67b87f8dfc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-67b87f8dfc-9c6k4"
I1028 08:53:48.896348       1 event.go:294] "Event occurred" object="default/postgres14" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres14-84d9c9577f to 1"
I1028 08:53:48.939614       1 event.go:294] "Event occurred" object="default/postgres14-84d9c9577f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres14-84d9c9577f-cg96k"
I1028 08:53:51.589847       1 event.go:294] "Event occurred" object="default/postgres14" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgres14-6694bfc79 to 0 from 1"
I1028 08:53:51.817183       1 event.go:294] "Event occurred" object="default/postgres14-6694bfc79" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgres14-6694bfc79-f2f7w"
W1028 08:55:24.021356       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/msvc-cursos", retrying. Error: EndpointSlice informer cache is out of date
I1028 08:55:29.905638       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-cursos-56fc646c96 to 1"
I1028 08:55:29.945723       1 event.go:294] "Event occurred" object="default/msvc-cursos-56fc646c96" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-cursos-56fc646c96-lvz7x"
I1028 11:20:33.979465       1 event.go:294] "Event occurred" object="default/postgres14" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres14-6dd94f9766 to 1"
I1028 11:20:34.108239       1 event.go:294] "Event occurred" object="default/postgres14-6dd94f9766" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres14-6dd94f9766-d876l"
I1028 11:20:37.477178       1 event.go:294] "Event occurred" object="default/postgres14" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgres14-84d9c9577f to 0 from 1"
I1028 11:20:37.481986       1 event.go:294] "Event occurred" object="default/postgres14-84d9c9577f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgres14-84d9c9577f-cg96k"
I1028 11:20:39.347465       1 event.go:294] "Event occurred" object="default/mysql8" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mysql8-66649767dd to 1"
I1028 11:20:39.356053       1 event.go:294] "Event occurred" object="default/mysql8-66649767dd" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mysql8-66649767dd-rkzjt"
I1028 11:20:43.410402       1 event.go:294] "Event occurred" object="default/mysql8" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set mysql8-6686564bb5 to 0 from 1"
I1028 11:20:43.449803       1 event.go:294] "Event occurred" object="default/mysql8-6686564bb5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: mysql8-6686564bb5-zt9sr"
I1028 11:20:48.274941       1 event.go:294] "Event occurred" object="default/msvc-cursos" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-cursos-549755bdf5 to 1"
I1028 11:20:48.720637       1 event.go:294] "Event occurred" object="default/msvc-cursos-549755bdf5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-cursos-549755bdf5-xmdpr"
I1028 11:20:54.616966       1 event.go:294] "Event occurred" object="default/msvc-usuarios" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set msvc-usuarios-8c845f9b8 to 1"
I1028 11:20:54.650389       1 event.go:294] "Event occurred" object="default/msvc-usuarios-8c845f9b8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-8c845f9b8-446wj"
W1028 11:20:58.143553       1 endpointslice_controller.go:306] Error syncing endpoint slices for service "default/mysql8", retrying. Error: EndpointSlice informer cache is out of date
I1028 11:21:53.218757       1 event.go:294] "Event occurred" object="default/msvc-cursos-549755bdf5" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-cursos-549755bdf5-7brj5"
I1028 11:22:05.070788       1 event.go:294] "Event occurred" object="default/msvc-usuarios-8c845f9b8" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: msvc-usuarios-8c845f9b8-f2wqr"
W1101 13:34:44.866590       1 garbagecollector.go:754] failed to discover preferred resources: Unauthorized
E1101 13:34:44.866633       1 resource_quota_controller.go:417] failed to discover resources: Unauthorized

* 
* ==> kube-proxy [3a4d4acfe785] <==
* I1027 01:16:01.414596       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1027 01:16:01.414750       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1027 01:16:01.414783       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1027 01:16:02.598039       1 server_others.go:206] "Using iptables Proxier"
I1027 01:16:02.598121       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1027 01:16:02.598136       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1027 01:16:02.598170       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1027 01:16:02.660108       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1027 01:16:02.660352       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1027 01:16:02.660813       1 server.go:661] "Version info" version="v1.25.2"
I1027 01:16:02.660833       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1027 01:16:02.754664       1 config.go:317] "Starting service config controller"
I1027 01:16:02.754726       1 shared_informer.go:255] Waiting for caches to sync for service config
I1027 01:16:02.754800       1 config.go:226] "Starting endpoint slice config controller"
I1027 01:16:02.754821       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1027 01:16:02.755151       1 config.go:444] "Starting node config controller"
I1027 01:16:02.755176       1 shared_informer.go:255] Waiting for caches to sync for node config
I1027 01:16:02.855841       1 shared_informer.go:262] Caches are synced for node config
I1027 01:16:02.855976       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1027 01:16:03.075687       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-proxy [6bb1b0f8193b] <==
* I1101 13:53:55.574398       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1101 13:53:55.574505       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1101 13:53:55.574539       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1101 13:53:55.993171       1 server_others.go:206] "Using iptables Proxier"
I1101 13:53:55.993363       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1101 13:53:55.993402       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1101 13:53:55.993417       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1101 13:53:56.005084       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1101 13:53:56.005430       1 proxier.go:262] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1101 13:53:56.005733       1 server.go:661] "Version info" version="v1.25.2"
I1101 13:53:56.005765       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1101 13:53:56.030402       1 config.go:226] "Starting endpoint slice config controller"
I1101 13:53:56.030535       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1101 13:53:56.030659       1 config.go:317] "Starting service config controller"
I1101 13:53:56.030675       1 shared_informer.go:255] Waiting for caches to sync for service config
I1101 13:53:56.030710       1 config.go:444] "Starting node config controller"
I1101 13:53:56.030718       1 shared_informer.go:255] Waiting for caches to sync for node config
I1101 13:53:56.131679       1 shared_informer.go:262] Caches are synced for node config
I1101 13:53:56.131786       1 shared_informer.go:262] Caches are synced for service config
I1101 13:53:56.131885       1 shared_informer.go:262] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [208a5171c120] <==
* E1027 01:15:42.481599       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.626992       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.627042       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.681405       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.681438       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.700598       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.700650       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.745070       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.745120       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.770417       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.770551       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.804232       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.804287       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.844015       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.844064       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:42.896476       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1027 01:15:42.896506       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1027 01:15:47.510950       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1027 01:15:47.511004       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1027 01:15:47.511024       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1027 01:15:47.511038       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1027 01:15:47.511098       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1027 01:15:47.511122       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1027 01:15:47.511108       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1027 01:15:47.511150       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1027 01:15:47.511170       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1027 01:15:47.511181       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1027 01:15:47.511227       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1027 01:15:47.511253       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1027 01:15:47.511182       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1027 01:15:47.511273       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1027 01:15:47.511629       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1027 01:15:47.511674       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1027 01:15:47.511804       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1027 01:15:47.511853       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1027 01:15:47.512083       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1027 01:15:47.512259       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1027 01:15:47.512266       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1027 01:15:47.512429       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1027 01:15:47.512155       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1027 01:15:47.512453       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1027 01:15:47.512285       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1027 01:15:47.512474       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1027 01:15:47.512109       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1027 01:15:47.512693       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1027 01:15:47.512371       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1027 01:15:47.512961       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1027 01:15:52.871352       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1027 15:48:48.931724       1 trace.go:205] Trace[1397870276]: "Scheduling" namespace:default,name:msvc-usuarios-67b87f8dfc-zj5rb (27-Oct-2022 15:48:48.725) (total time: 147ms):
Trace[1397870276]: ---"Snapshotting scheduler cache and node infos done" 35ms (15:48:48.761)
Trace[1397870276]: ---"Computing predicates done" 111ms (15:48:48.872)
Trace[1397870276]: [147.403257ms] [147.403257ms] END
I1028 08:44:40.780755       1 trace.go:205] Trace[1893772748]: "Scheduling" namespace:default,name:mysql8-6686564bb5-zt9sr (28-Oct-2022 08:44:40.533) (total time: 160ms):
Trace[1893772748]: ---"Computing predicates done" 160ms (08:44:40.694)
Trace[1893772748]: [160.889343ms] [160.889343ms] END
I1101 13:40:57.109719       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I1101 13:40:57.109760       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1101 13:40:57.116534       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
E1101 13:40:57.218527       1 scheduling_queue.go:963] "Error while retrieving next pod from scheduling queue" err="scheduling queue is closed"
E1101 13:40:59.222862       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [50dea40ba47e] <==
* E1101 13:53:15.616580       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:15.700549       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:15.700598       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:15.730692       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:15.731180       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:15.738831       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:15.738880       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:15.855625       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:15.855697       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:15.911079       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:15.911143       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:15.920883       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:15.920962       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:16.082458       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:16.082512       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:16.291625       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:16.291855       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:16.321064       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:16.321139       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:16.847888       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:16.847960       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:18.958489       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:18.958546       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:19.143331       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:19.143426       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:19.361062       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:19.361117       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:19.407392       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:19.407444       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:20.313363       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:20.313435       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:20.379132       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:20.379192       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:20.467584       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1101 13:53:20.467635       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1101 13:53:27.955197       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1101 13:53:27.955273       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1101 13:53:27.955336       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1101 13:53:27.955354       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1101 13:53:27.955379       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1101 13:53:27.955487       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1101 13:53:27.955386       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1101 13:53:27.955736       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1101 13:53:27.955447       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1101 13:53:27.955839       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1101 13:53:27.955892       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1101 13:53:27.955857       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1101 13:53:27.955280       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1101 13:53:27.955774       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1101 13:53:27.955925       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1101 13:53:27.955791       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1101 13:53:27.955965       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1101 13:53:27.955455       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1101 13:53:27.956233       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1101 13:53:27.956260       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1101 13:53:27.956261       1 reflector.go:424] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1101 13:53:27.956312       1 reflector.go:140] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1101 13:53:27.968480       1 reflector.go:424] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1101 13:53:27.968522       1 reflector.go:140] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1101 13:53:35.025554       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Tue 2022-11-01 13:52:28 UTC, end at Tue 2022-11-01 13:56:50 UTC. --
Nov 01 13:53:28 minikube kubelet[1043]: I1101 13:53:28.486821    1043 topology_manager.go:205] "Topology Admit Handler"
Nov 01 13:53:28 minikube kubelet[1043]: I1101 13:53:28.486890    1043 topology_manager.go:205] "Topology Admit Handler"
Nov 01 13:53:29 minikube kubelet[1043]: I1101 13:53:29.016354    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"postgres-pv\" (UniqueName: \"kubernetes.io/host-path/c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53-postgres-pv\") pod \"postgres14-6dd94f9766-d876l\" (UID: \"c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53\") " pod="default/postgres14-6dd94f9766-d876l"
Nov 01 13:53:29 minikube kubelet[1043]: I1101 13:53:29.016451    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-865dh\" (UniqueName: \"kubernetes.io/projected/c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53-kube-api-access-865dh\") pod \"postgres14-6dd94f9766-d876l\" (UID: \"c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53\") " pod="default/postgres14-6dd94f9766-d876l"
Nov 01 13:53:29 minikube kubelet[1043]: I1101 13:53:29.016505    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-9dx7z\" (UniqueName: \"kubernetes.io/projected/5515731c-cfee-4af0-a227-3bf01279d481-kube-api-access-9dx7z\") pod \"coredns-565d847f94-r4f4l\" (UID: \"5515731c-cfee-4af0-a227-3bf01279d481\") " pod="kube-system/coredns-565d847f94-r4f4l"
Nov 01 13:53:29 minikube kubelet[1043]: I1101 13:53:29.016552    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/5515731c-cfee-4af0-a227-3bf01279d481-config-volume\") pod \"coredns-565d847f94-r4f4l\" (UID: \"5515731c-cfee-4af0-a227-3bf01279d481\") " pod="kube-system/coredns-565d847f94-r4f4l"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.449780    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/7550851a-626d-4cf4-950f-c35337808b09-xtables-lock\") pod \"kube-proxy-cgglk\" (UID: \"7550851a-626d-4cf4-950f-c35337808b09\") " pod="kube-system/kube-proxy-cgglk"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.449874    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/7550851a-626d-4cf4-950f-c35337808b09-lib-modules\") pod \"kube-proxy-cgglk\" (UID: \"7550851a-626d-4cf4-950f-c35337808b09\") " pod="kube-system/kube-proxy-cgglk"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.449931    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/c64c1953-f284-44d6-9c82-27cfc6cda6c9-tmp\") pod \"storage-provisioner\" (UID: \"c64c1953-f284-44d6-9c82-27cfc6cda6c9\") " pod="kube-system/storage-provisioner"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.449982    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/7550851a-626d-4cf4-950f-c35337808b09-kube-proxy\") pod \"kube-proxy-cgglk\" (UID: \"7550851a-626d-4cf4-950f-c35337808b09\") " pod="kube-system/kube-proxy-cgglk"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.450045    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tbmhx\" (UniqueName: \"kubernetes.io/projected/7550851a-626d-4cf4-950f-c35337808b09-kube-api-access-tbmhx\") pod \"kube-proxy-cgglk\" (UID: \"7550851a-626d-4cf4-950f-c35337808b09\") " pod="kube-system/kube-proxy-cgglk"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.450164    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xdsbh\" (UniqueName: \"kubernetes.io/projected/6f90357d-9c2f-42c0-95e7-ad0df272234b-kube-api-access-xdsbh\") pod \"mysql8-66649767dd-rkzjt\" (UID: \"6f90357d-9c2f-42c0-95e7-ad0df272234b\") " pod="default/mysql8-66649767dd-rkzjt"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.450229    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"mysql-pv\" (UniqueName: \"kubernetes.io/host-path/6f90357d-9c2f-42c0-95e7-ad0df272234b-mysql-pv\") pod \"mysql8-66649767dd-rkzjt\" (UID: \"6f90357d-9c2f-42c0-95e7-ad0df272234b\") " pod="default/mysql8-66649767dd-rkzjt"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.450302    1043 reconciler.go:357] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-phl4q\" (UniqueName: \"kubernetes.io/projected/c64c1953-f284-44d6-9c82-27cfc6cda6c9-kube-api-access-phl4q\") pod \"storage-provisioner\" (UID: \"c64c1953-f284-44d6-9c82-27cfc6cda6c9\") " pod="kube-system/storage-provisioner"
Nov 01 13:53:30 minikube kubelet[1043]: I1101 13:53:30.450335    1043 reconciler.go:169] "Reconciler: start to sync state"
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.171209    1043 kuberuntime_manager.go:862] container &Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.9.3,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-9dx7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod coredns-565d847f94-r4f4l_kube-system(5515731c-cfee-4af0-a227-3bf01279d481): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.171281    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/coredns-565d847f94-r4f4l" podUID=5515731c-cfee-4af0-a227-3bf01279d481
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.278363    1043 kuberuntime_manager.go:862] container &Container{Name:postgres14,Image:postgres:14-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-cursos,},Key:password,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:POSTGRES_DB,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-usuarios,},Key:database,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:data-postgres,ReadOnly:false,MountPath:/var/lib/postgresql/data,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-865dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod postgres14-6dd94f9766-d876l_default(c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.278442    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres14\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="default/postgres14-6dd94f9766-d876l" podUID=c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53
Nov 01 13:53:32 minikube kubelet[1043]: I1101 13:53:32.279875    1043 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="bd51e348bca7569841df60446f66c6d6dc168f2291e21163513c1f8e1bd3c93d"
Nov 01 13:53:32 minikube kubelet[1043]: I1101 13:53:32.290822    1043 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="865321f5bf0874c48ebd1b2e2fab388f4ab9c4c37656059956db4a164bf5fa42"
Nov 01 13:53:32 minikube kubelet[1043]: I1101 13:53:32.291174    1043 scope.go:115] "RemoveContainer" containerID="7d0850c601f0c578f2ba3962f579f179ef847f9052d41bbd427474b156713ed6"
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.295380    1043 kuberuntime_manager.go:862] container &Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.9.3,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-9dx7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod coredns-565d847f94-r4f4l_kube-system(5515731c-cfee-4af0-a227-3bf01279d481): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.295468    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/coredns-565d847f94-r4f4l" podUID=5515731c-cfee-4af0-a227-3bf01279d481
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.571392    1043 kuberuntime_manager.go:862] container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-phl4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod storage-provisioner_kube-system(c64c1953-f284-44d6-9c82-27cfc6cda6c9): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.571456    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID=c64c1953-f284-44d6-9c82-27cfc6cda6c9
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.622578    1043 kuberuntime_manager.go:862] container &Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.25.2,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-tbmhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod kube-proxy-cgglk_kube-system(7550851a-626d-4cf4-950f-c35337808b09): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.622640    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-cgglk" podUID=7550851a-626d-4cf4-950f-c35337808b09
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.922893    1043 kuberuntime_manager.go:862] container &Container{Name:mysql,Image:mysql:8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-usuarios,},Key:password,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-usuarios,},Key:database,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:data-mysql,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-xdsbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod mysql8-66649767dd-rkzjt_default(6f90357d-9c2f-42c0-95e7-ad0df272234b): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:32 minikube kubelet[1043]: E1101 13:53:32.922968    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="default/mysql8-66649767dd-rkzjt" podUID=6f90357d-9c2f-42c0-95e7-ad0df272234b
Nov 01 13:53:33 minikube kubelet[1043]: I1101 13:53:33.313538    1043 scope.go:115] "RemoveContainer" containerID="87074a63a0e2f6bae18f5ca9393ccb31d291d4dc47e6ced7d7717ce8c538b16b"
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.315544    1043 kuberuntime_manager.go:862] container &Container{Name:storage-provisioner,Image:gcr.io/k8s-minikube/storage-provisioner:v5,Command:[/storage-provisioner],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:tmp,ReadOnly:false,MountPath:/tmp,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-phl4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod storage-provisioner_kube-system(c64c1953-f284-44d6-9c82-27cfc6cda6c9): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.315614    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/storage-provisioner" podUID=c64c1953-f284-44d6-9c82-27cfc6cda6c9
Nov 01 13:53:33 minikube kubelet[1043]: I1101 13:53:33.322075    1043 scope.go:115] "RemoveContainer" containerID="7d0850c601f0c578f2ba3962f579f179ef847f9052d41bbd427474b156713ed6"
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.325284    1043 kuberuntime_manager.go:862] container &Container{Name:coredns,Image:registry.k8s.io/coredns/coredns:v1.9.3,Command:[],Args:[-conf /etc/coredns/Corefile],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:dns,HostPort:0,ContainerPort:53,Protocol:UDP,HostIP:,},ContainerPort{Name:dns-tcp,HostPort:0,ContainerPort:53,Protocol:TCP,HostIP:,},ContainerPort{Name:metrics,HostPort:0,ContainerPort:9153,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{memory: {{178257920 0} {<nil>} 170Mi BinarySI},},Requests:ResourceList{cpu: {{100 -3} {<nil>} 100m DecimalSI},memory: {{73400320 0} {<nil>} 70Mi BinarySI},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:config-volume,ReadOnly:true,MountPath:/etc/coredns,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-9dx7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:{0 8080 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,TerminationGracePeriodSeconds:nil,},ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/ready,Port:{0 8181 },Host:,Scheme:HTTP,HTTPHeaders:[]HTTPHeader{},},TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,},Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[NET_BIND_SERVICE],Drop:[all],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:*true,AllowPrivilegeEscalation:*false,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod coredns-565d847f94-r4f4l_kube-system(5515731c-cfee-4af0-a227-3bf01279d481): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.325526    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"coredns\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/coredns-565d847f94-r4f4l" podUID=5515731c-cfee-4af0-a227-3bf01279d481
Nov 01 13:53:33 minikube kubelet[1043]: I1101 13:53:33.332204    1043 scope.go:115] "RemoveContainer" containerID="3a4d4acfe7851c31ed899ab723de8af72fa26c45e4ba78a36888c19cbbae8dfe"
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.334623    1043 kuberuntime_manager.go:862] container &Container{Name:kube-proxy,Image:registry.k8s.io/kube-proxy:v1.25.2,Command:[/usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=$(NODE_NAME)],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-proxy,ReadOnly:false,MountPath:/var/lib/kube-proxy,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:xtables-lock,ReadOnly:false,MountPath:/run/xtables.lock,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:lib-modules,ReadOnly:true,MountPath:/lib/modules,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-tbmhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod kube-proxy-cgglk_kube-system(7550851a-626d-4cf4-950f-c35337808b09): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.334698    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="kube-system/kube-proxy-cgglk" podUID=7550851a-626d-4cf4-950f-c35337808b09
Nov 01 13:53:33 minikube kubelet[1043]: I1101 13:53:33.341173    1043 scope.go:115] "RemoveContainer" containerID="29c56f864d352b48135be885f858e04e682172f6109845d9d20fbdb780b1e77d"
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.343996    1043 kuberuntime_manager.go:862] container &Container{Name:mysql,Image:mysql:8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3306,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MYSQL_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-usuarios,},Key:password,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:MYSQL_DATABASE,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-usuarios,},Key:database,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:data-mysql,ReadOnly:false,MountPath:/var/lib/mysql,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-xdsbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod mysql8-66649767dd-rkzjt_default(6f90357d-9c2f-42c0-95e7-ad0df272234b): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.344275    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mysql\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="default/mysql8-66649767dd-rkzjt" podUID=6f90357d-9c2f-42c0-95e7-ad0df272234b
Nov 01 13:53:33 minikube kubelet[1043]: I1101 13:53:33.352999    1043 scope.go:115] "RemoveContainer" containerID="5598d642bbc22e9ddeaa4f0860620f85f27c356bcd324641036c8799370f9530"
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.356073    1043 kuberuntime_manager.go:862] container &Container{Name:postgres14,Image:postgres:14-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:5432,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:POSTGRES_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-cursos,},Key:password,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:POSTGRES_DB,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:msvc-usuarios,},Key:database,Optional:nil,},SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:data-postgres,ReadOnly:false,MountPath:/var/lib/postgresql/data,SubPath:,MountPropagation:nil,SubPathExpr:,},VolumeMount{Name:kube-api-access-865dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,} start failed in pod postgres14-6dd94f9766-d876l_default(c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars
Nov 01 13:53:33 minikube kubelet[1043]: E1101 13:53:33.356132    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"postgres14\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="default/postgres14-6dd94f9766-d876l" podUID=c6ee2a50-f2f8-4d13-a1d9-a6656b7f6e53
Nov 01 13:53:34 minikube kubelet[1043]: I1101 13:53:34.376295    1043 scope.go:115] "RemoveContainer" containerID="5598d642bbc22e9ddeaa4f0860620f85f27c356bcd324641036c8799370f9530"
Nov 01 13:53:34 minikube kubelet[1043]: I1101 13:53:34.376334    1043 scope.go:115] "RemoveContainer" containerID="7d0850c601f0c578f2ba3962f579f179ef847f9052d41bbd427474b156713ed6"
Nov 01 13:53:47 minikube kubelet[1043]: I1101 13:53:47.875328    1043 scope.go:115] "RemoveContainer" containerID="29c56f864d352b48135be885f858e04e682172f6109845d9d20fbdb780b1e77d"
Nov 01 13:53:48 minikube kubelet[1043]: I1101 13:53:48.866183    1043 scope.go:115] "RemoveContainer" containerID="87074a63a0e2f6bae18f5ca9393ccb31d291d4dc47e6ced7d7717ce8c538b16b"
Nov 01 13:53:48 minikube kubelet[1043]: I1101 13:53:48.866403    1043 scope.go:115] "RemoveContainer" containerID="3a4d4acfe7851c31ed899ab723de8af72fa26c45e4ba78a36888c19cbbae8dfe"
Nov 01 13:54:22 minikube kubelet[1043]: I1101 13:54:22.956572    1043 scope.go:115] "RemoveContainer" containerID="87074a63a0e2f6bae18f5ca9393ccb31d291d4dc47e6ced7d7717ce8c538b16b"
Nov 01 13:54:22 minikube kubelet[1043]: I1101 13:54:22.956848    1043 scope.go:115] "RemoveContainer" containerID="7c168047037b7ee03948770c7ed24b0349a05e56ed2b5e629b81f5c86dac7372"
Nov 01 13:54:22 minikube kubelet[1043]: E1101 13:54:22.967365    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c64c1953-f284-44d6-9c82-27cfc6cda6c9)\"" pod="kube-system/storage-provisioner" podUID=c64c1953-f284-44d6-9c82-27cfc6cda6c9
Nov 01 13:54:33 minikube kubelet[1043]: I1101 13:54:33.865782    1043 scope.go:115] "RemoveContainer" containerID="7c168047037b7ee03948770c7ed24b0349a05e56ed2b5e629b81f5c86dac7372"
Nov 01 13:54:33 minikube kubelet[1043]: E1101 13:54:33.866127    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c64c1953-f284-44d6-9c82-27cfc6cda6c9)\"" pod="kube-system/storage-provisioner" podUID=c64c1953-f284-44d6-9c82-27cfc6cda6c9
Nov 01 13:54:45 minikube kubelet[1043]: I1101 13:54:45.865899    1043 scope.go:115] "RemoveContainer" containerID="7c168047037b7ee03948770c7ed24b0349a05e56ed2b5e629b81f5c86dac7372"
Nov 01 13:54:45 minikube kubelet[1043]: E1101 13:54:45.866075    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c64c1953-f284-44d6-9c82-27cfc6cda6c9)\"" pod="kube-system/storage-provisioner" podUID=c64c1953-f284-44d6-9c82-27cfc6cda6c9
Nov 01 13:54:57 minikube kubelet[1043]: I1101 13:54:57.865812    1043 scope.go:115] "RemoveContainer" containerID="7c168047037b7ee03948770c7ed24b0349a05e56ed2b5e629b81f5c86dac7372"
Nov 01 13:54:57 minikube kubelet[1043]: E1101 13:54:57.866031    1043 pod_workers.go:965] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 40s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(c64c1953-f284-44d6-9c82-27cfc6cda6c9)\"" pod="kube-system/storage-provisioner" podUID=c64c1953-f284-44d6-9c82-27cfc6cda6c9
Nov 01 13:55:12 minikube kubelet[1043]: I1101 13:55:12.865379    1043 scope.go:115] "RemoveContainer" containerID="7c168047037b7ee03948770c7ed24b0349a05e56ed2b5e629b81f5c86dac7372"

* 
* ==> storage-provisioner [7c168047037b] <==
* I1101 13:53:52.492637       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1101 13:54:22.652575       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [f9f531c44e92] <==
* I1101 13:55:13.859244       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1101 13:55:14.403635       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1101 13:55:14.403781       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1101 13:55:31.847296       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1101 13:55:31.847581       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_55874ba3-4186-40c7-8a3b-e12dcfb37834!
I1101 13:55:31.848217       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"824dbd8d-1d29-42f3-9019-46298a5fbe5f", APIVersion:"v1", ResourceVersion:"115342", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_55874ba3-4186-40c7-8a3b-e12dcfb37834 became leader
I1101 13:55:31.974084       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_55874ba3-4186-40c7-8a3b-e12dcfb37834!

